{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors (KNN) Algorithm\n",
    "\n",
    "k-Nearest Neighbors (k-NN) is a supervised learning algorithm that can be used for classification and regression tasks. It is a lazy learning algorithm, as it does not build an explicit model during the training phase, but instead stores the entire training dataset and makes predictions based on a similarity measure.\n",
    "\n",
    "## History\n",
    "\n",
    "The k-NN algorithm dates back to the early 1950s, with the work of Fix and Hodges on pattern recognition. The algorithm became more popular in the 1960s and 1970s due to its simplicity and effectiveness in a variety of tasks.\n",
    "\n",
    "## Mathematical Equations\n",
    "\n",
    "k-NN does not involve a specific mathematical equation. The algorithm is based on a similarity measure, usually the Euclidean distance, which is calculated between data points:\n",
    "\n",
    "Euclidean distance = sqrt(Î£(x_i - y_i)^2)\n",
    "\n",
    "where x_i and y_i are the coordinates of the data points x and y, respectively.\n",
    "\n",
    "## Learning Algorithm\n",
    "\n",
    "The learning algorithm for k-NN consists of the following steps:\n",
    "\n",
    "1. Determine the value of k (number of nearest neighbors) and the distance metric.\n",
    "2. For each new data point:\n",
    "    a. Calculate the distance between the new data point and all the training data points.\n",
    "    b. Find the k training data points with the smallest distances.\n",
    "    c. For classification, predict the class that has the majority vote among the k nearest neighbors.\n",
    "       For regression, predict the average target value of the k nearest neighbors.\n",
    "\n",
    "## Pros and Cons\n",
    "\n",
    "**Pros:**\n",
    "- Simple to understand and implement.\n",
    "- Can adapt to the data as it does not make any assumptions about the underlying data distribution.\n",
    "- Works well with small datasets.\n",
    "\n",
    "**Cons:**\n",
    "- Computationally expensive, especially for large datasets, as it needs to calculate the distance between the new data point and all the training data points.\n",
    "- Sensitive to the choice of k and the distance metric.\n",
    "- Does not work well with high-dimensional data (curse of dimensionality).\n",
    "- Requires preprocessing (e.g., scaling) to ensure that all features have equal importance.\n",
    "\n",
    "## Suitable Tasks and Datasets\n",
    "\n",
    "k-NN can be applied to a variety of classification and regression tasks, including:\n",
    "\n",
    "- Handwritten digit recognition\n",
    "- Image classification\n",
    "- Recommender systems\n",
    "- Anomaly detection\n",
    "\n",
    "It works well with small datasets and low-dimensional data. k-NN is not suitable for large datasets or high-dimensional data due to the computational cost and the curse of dimensionality.\n",
    "\n",
    "## References\n",
    "\n",
    "1. Fix, E., & Hodges Jr, J. L. (1951). Discriminatory analysis. Nonparametric discrimination: Consistency properties. Technical Report 4, USAF School of Aviation Medicine.\n",
    "2. Cover, T., & Hart, P. (1967). Nearest neighbor pattern classification. IEEE Transactions on Information Theory, 13(1), 21-27.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from collections import Counter\n",
    "\n",
    "# Function to calculate Euclidean distance between two points\n",
    "def euclidean_distance(a, b):\n",
    "    return np.sqrt(np.sum((a - b) ** 2))\n",
    "\n",
    "# KNN class\n",
    "class KNN:\n",
    "    def __init__(self, k=3):\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = [self._predict(x) for x in X]\n",
    "        return np.array(y_pred)\n",
    "\n",
    "    def _predict(self, x):\n",
    "        distances = [euclidean_distance(x, x_train) for x_train in self.X_train]\n",
    "        k_indices = np.argsort(distances)[:self.k]\n",
    "        k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
    "        most_common = Counter(k_nearest_labels).most_common(1)\n",
    "        return most_common[0][0]\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply the k-NN algorithm\n",
    "knn = KNN(k=3)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "# Visualize the results (using only the first two features)\n",
    "X_train_2d, X_test_2d = X_train[:, :2], X_test[:, :2]\n",
    "knn.fit(X_train_2d, y_train)\n",
    "y_pred_2d = knn.predict(X_test_2d)\n",
    "\n",
    "plt.scatter(X_test_2d[:, 0], X_test_2d[:, 1], c=y_pred_2d, cmap='viridis', marker='o', label='Predicted')\n",
    "plt.scatter(X_train_2d[:, 0], X_train_2d[:, 1], c=y_train, cmap='viridis', marker='x', label='Training')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.title('k-NN Classification (Iris Dataset)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
