{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron\n",
    "##### Author: Chenyang Skylar Li\n",
    "\n",
    "# Table of Contents\n",
    "\n",
    "    \n",
    "## Introduction\n",
    "The Perceptron, one of the earliest machine learning models, is a binary classification algorithm that was first introduced by Frank Rosenblatt in 1957. Inspired by the workings of biological neurons, Rosenblatt proposed the algorithm while he was at the Cornell Aeronautical Laboratory with an aim to create a simple model for pattern recognition tasks. As a type of artificial neuron, the Perceptron is primarily used for linearly separable datasets and serves as the foundational building block for many modern neural networks.\n",
    "\n",
    "## Mathematical Fundations\n",
    "\n",
    "The Perceptron computes a linear combination of input features and a bias term. The resulting value is passed through an activation function to produce the output.\n",
    "\n",
    "`output = activation(sum(w_i * x_i) + b)`\n",
    "\n",
    "The common activation function used in Perceptrons is the Heaviside step function or sign function:\n",
    "\n",
    "![activation function](/assests/images/perceptron_activation_function.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Weight Update Rule\n",
    "\n",
    "If an instance is misclassified, update the weights and bias as follows:\n",
    "\n",
    "`w_i = w_i + learning_rate * (target - output) * x_i`\n",
    "\n",
    "`b = b + learning_rate * (target - output)`\n",
    "\n",
    "## Learning Algorithm\n",
    "\n",
    "1. Initialize the weights and bias to zero or small random values.\n",
    "2. For each training instance:\n",
    "   - Compute the output using the activation function.\n",
    "   - Update the weights and bias if the output is not correct.\n",
    "3. Repeat step 2 for the desired number of epochs or until convergence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define a Perceptron class\n",
    "class Perceptron:\n",
    "    \"\"\"\n",
    "    Perceptron(n_features, learning_rate=0.01, epochs=1000)\n",
    "    \n",
    "    Perceptron, -A simple binary classification algorithm that learns a linear decision boundary between two classes.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_features : int\n",
    "        The number of input features for each training example.\n",
    "    learning_rate : float, optional (default=0.01)\n",
    "        The learning rate used to update the weights and bias during training.\n",
    "    epochs : int, optional (default=1000)\n",
    "        The number of iterations to train the model for.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    weights : array-like, shape (n_features,)\n",
    "        The weights learned by the perceptron during training.\n",
    "    bias : float\n",
    "        The bias learned by the perceptron during training.\n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    fit(X, y)\n",
    "        Train the perceptron on the given training data (input vectors X and target outputs y).\n",
    "    predict(X)\n",
    "        Make predictions for the given input vectors X based on the learned weights and bias.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Constructor for the Perceptron class\n",
    "    def __init__(self, n_features, learning_rate=0.01, epochs=1000):\n",
    "        self.n_features = n_features\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        # self.weights = np.zeros(n_features)  # Use this line to replace below line if you want to initialize the weights to zero\n",
    "        self.weights = np.random.randn(n_features)  # Initialize the weights randomly\n",
    "        self.bias = 0  # Initialize the bias to zero\n",
    "        \n",
    "    # Define the predict method for the Perceptron class\n",
    "    def predict(self, X):\n",
    "        # Calculate the linear output by taking the dot product of the input X and the weights, and adding the bias\n",
    "        linear_output = np.dot(X, self.weights) + self.bias\n",
    "        # Apply the heaviside step function to the linear output to get the predicted class label (0 or 1)\n",
    "        return np.where(linear_output > 0, 1, 0)\n",
    "        # return np.sign(linear_output)  # Use this line to replace above line if you want to use the sign function instead of the heaviside step function\n",
    "    \n",
    "    # Define the fit method for the Perceptron class\n",
    "    def fit(self, X, y):\n",
    "        # Iterate through the specified number of epochs\n",
    "        for _ in range(self.epochs):\n",
    "            # Iterate through each training example (input vector) and its corresponding target output\n",
    "            for xi, target in zip(X, y):\n",
    "                # Make a prediction for the input vector\n",
    "                output = self.predict(xi)\n",
    "                # Calculate the update factor for the weights and bias based on the difference between the predicted output and the target output\n",
    "                update = self.learning_rate * (target - output)\n",
    "                # Update the weights and bias based on the update factor and the input vector\n",
    "                self.weights += update * xi\n",
    "                self.bias += update"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros and Cons\n",
    "\n",
    "**Pros:**\n",
    "- Simple and easy to implement.\n",
    "- Efficient for linearly separable data.\n",
    "- Can be used as a building block for more complex models (e.g., multi-layer perceptrons).\n",
    "\n",
    "**Cons:**\n",
    "- Cannot solve non-linearly separable problems.\n",
    "- Sensitive to the choice of learning rate and initial weights.\n",
    "- No guarantee of convergence for non-linearly separable data.\n",
    "\n",
    "## Suitable Tasks and Datasets\n",
    "\n",
    "The Perceptron algorithm is a binary classification model that is well-suited to certain types of tasks and datasets:\n",
    "\n",
    "1. **Binary Classification Tasks**: The Perceptron is designed to handle problems where each instance can be classified into one of two classes. This makes it useful for tasks such as classifying emails as spam or not spam, or classifying loan applicants as high risk or low risk.\n",
    "\n",
    "2. **Linearly Separable Data**: The Perceptron algorithm is most effective when the classes in the data can be separated by a hyperplane in the feature space, a condition known as linear separability. If you can draw a straight line (in 2D) or a plane (in 3D) to separate the two classes in the data, then the Perceptron will be able to learn this decision boundary.\n",
    "\n",
    "3. **Large Scale Datasets**: The Perceptron, due to its simplicity and efficiency, can effectively handle large scale datasets. It learns iteratively from the data, making it suitable for online learning tasks where data is processed sequentially.\n",
    "\n",
    "The Perceptron is best suited for linearly separable datasets and binary classification tasks. Some examples include:\n",
    "- Predicting whether an email is spam or not.\n",
    "- Classifying handwritten digits (0 and 1).\n",
    "- Separating two different types of plants based on their features.\n",
    "\n",
    "## References\n",
    "\n",
    "1. Rosenblatt, F. (1958). The perceptron: a probabilistic model for information storage and organization in the brain. Psychological review, 65(6), 386.\n",
    "2. Minsky, M., & Papert, S. (1969). An introduction to computational geometry. Cambridge tiass., HIT, 479, 480."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load the Iris dataset and prepare the binary classification data\n",
    "data = load_iris()\n",
    "X, y = data.data[:100], data.target[:100]\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the Perceptron\n",
    "perceptron = Perceptron(n_features=X_train.shape[1], learning_rate=0.01, epochs=1000)\n",
    "perceptron.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and calculate accuracy\n",
    "predictions = perceptron.predict(X_test)\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear output: [ 1 -2  3 -4  5  0]\n",
      "Predicted labels: [ 1 -1  1 -1  1  0]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([1, -2, 3, -4, 5, 0])\n",
    "\n",
    "\n",
    "# Apply the sign function to the linear output\n",
    "predicted_labels = np.sign(X)\n",
    "\n",
    "# Print the linear output and the predicted labels\n",
    "print(\"Linear output:\", X)\n",
    "print(\"Predicted labels:\", predicted_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
