{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "\n",
    "A decision tree is a tree-like structure used for both classification and regression tasks. In this model, the internal nodes represent features or attributes, branches represent decisions based on the values of these attributes, and leaves represent the final outcomes or target values.\n",
    "\n",
    "## History\n",
    "\n",
    "Decision trees have been used for decades in various fields, including data mining, pattern recognition, and artificial intelligence. Some of the earliest algorithms for decision tree construction include the Iterative Dichotomiser 3 (ID3), Classification and Regression Trees (CART), and Chi-squared Automatic Interaction Detection (CHAID).\n",
    "\n",
    "## Mathematical Concepts\n",
    "\n",
    "Decision trees use various mathematical concepts to determine the best splits at each node. Some of the most common criteria for measuring the quality of a split are:\n",
    "\n",
    "- Gini impurity: Measures the impurity or diversity of the samples in a node.\n",
    "- Information gain: Measures the reduction in entropy (disorder) after splitting the node.\n",
    "- Variance reduction: Measures the reduction in variance for regression tasks.\n",
    "\n",
    "## Learning Algorithm\n",
    "\n",
    "The learning algorithm for decision trees involves recursively partitioning the data into subsets based on the values of the input features. The main steps are:\n",
    "\n",
    "1. Start with the entire dataset at the root node.\n",
    "2. Select the best attribute to split the data based on a splitting criterion (e.g., Gini impurity, information gain, or variance reduction).\n",
    "3. Create child nodes for each value of the selected attribute.\n",
    "4. Recursively apply steps 2 and 3 to each child node until a stopping criterion is met (e.g., maximum tree depth, minimum samples per leaf, or no further improvement in the splitting criterion).\n",
    "\n",
    "## Pros and Cons\n",
    "\n",
    "**Pros:**\n",
    "\n",
    "- Easy to understand, interpret, and visualize.\n",
    "- Can handle both numerical and categorical data.\n",
    "- Requires little data preprocessing (e.g., no need for scaling or normalization).\n",
    "- Can be easily combined with other models to create ensemble methods (e.g., Random Forest, Gradient Boosting).\n",
    "\n",
    "**Cons:**\n",
    "\n",
    "- Prone to overfitting, especially when the tree is deep or has many leaves.\n",
    "- Can be unstable, as small changes in the data may lead to completely different trees.\n",
    "- May not perform well on datasets with complex relationships or high dimensionality.\n",
    "- Can be computationally expensive for large datasets.\n",
    "\n",
    "## Suitable Tasks and Datasets\n",
    "\n",
    "Decision trees can be applied to a wide range of classification and regression tasks where the relationship between the input features and the target variable can be modeled using a tree-like structure. Some examples include:\n",
    "\n",
    "- Diagnosing diseases based on symptoms.\n",
    "- Predicting customer churn.\n",
    "- Determining the optimal marketing strategy for a product.\n",
    "\n",
    "## References\n",
    "\n",
    "1. Quinlan, J. R. (1986). Induction of decision trees. Machine learning, 1(1), 81-106.\n",
    "2. Breiman, L., Friedman, J., Stone, C. J., & Olshen, R. A. (1984). Classification and regression trees. CRC press.\n",
    "3. Kass, G. V. (1980). An exploratory technique for investigating large quantities of categorical data. Journal of the Royal Statistical Society: Series C (Applied Statistics), 29(2), 119-127.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Helper function to calculate entropy\n",
    "def entropy(y):\n",
    "    _, counts = np.unique(y, return_counts=True)\n",
    "    probabilities = counts / len(y)\n",
    "    return -np.sum(probabilities * np.log2(probabilities))\n",
    "\n",
    "class DecisionTreeNode:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, min_samples_split=2, max_depth=None):\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.root = self._build_tree(X, y, depth=0)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return [self._predict_sample(x, self.root) for x in X]\n",
    "\n",
    "    def _build_tree(self, X, y, depth):\n",
    "        n_samples, n_features = X.shape\n",
    "        n_labels = len(np.unique(y))\n",
    "\n",
    "        # Stopping criteria\n",
    "        if (n_samples < self.min_samples_split\n",
    "                or n_labels == 1\n",
    "                or (self.max_depth is not None and depth >= self.max_depth)):\n",
    "            value = np.argmax(np.bincount(y))\n",
    "            return DecisionTreeNode(value=value)\n",
    "\n",
    "        # Find the best split\n",
    "        best_feature, best_threshold = self._find_best_split(X, y)\n",
    "        left_indices, right_indices = self._split(X[:, best_feature], best_threshold)\n",
    "\n",
    "        # Recursively build the left and right subtrees\n",
    "        left = self._build_tree(X[left_indices, :], y[left_indices], depth + 1)\n",
    "        right = self._build_tree(X[right_indices, :], y[right_indices], depth + 1)\n",
    "\n",
    "        return DecisionTreeNode(feature=best_feature, threshold=best_threshold, left=left, right=right)\n",
    "\n",
    "    def _find_best_split(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        best_feature, best_threshold, best_info_gain = None, None, 0\n",
    "\n",
    "        for feature in range(n_features):\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            for threshold in thresholds:\n",
    "                left_indices, right_indices = self._split(X[:, feature], threshold)\n",
    "                left_entropy = entropy(y[left_indices])\n",
    "                right_entropy = entropy(y[right_indices])\n",
    "                info_gain = entropy(y) - (len(left_indices) * left_entropy + len(right_indices) * right_entropy) / n_samples\n",
    "\n",
    "                if info_gain > best_info_gain:\n",
    "                    best_feature, best_threshold, best_info_gain = feature, threshold, info_gain\n",
    "\n",
    "        return best_feature, best_threshold\n",
    "\n",
    "    def _split(self, X_feature, threshold):\n",
    "        left_indices = np.where(X_feature <= threshold)[0]\n",
    "        right_indices = np.where(X_feature > threshold)[0]\n",
    "        return left_indices, right_indices\n",
    "\n",
    "    def _predict_sample(self, x, node):\n",
    "        if node.value is not None:\n",
    "            return node.value\n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self._predict_sample(x, node.left)\n",
    "        return self._predict_sample(x, node.right)\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the decision tree model\n",
    "model = DecisionTree(min_samples_split=2, max_depth=3)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy and confusion matrix\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_mat)\n",
    "\n",
    "# Visualize the decision tree using the print_tree function\n",
    "def print_tree(node, feature_names, class_names, prefix=\"\"):\n",
    "    if node.value is not None:\n",
    "        class_name = class_names[node.value]\n",
    "        print(f\"{prefix}Class: {class_name}\")\n",
    "    else:\n",
    "        feature_name = feature_names[node.feature]\n",
    "        threshold = node.threshold\n",
    "        print(f\"{prefix}{feature_name} <= {threshold:.2f}\")\n",
    "        print_tree(node.left, feature_names, class_names, prefix + \"\\t\")\n",
    "        print(f\"{prefix}{feature_name} > {threshold:.2f}\")\n",
    "        print_tree(node.right, feature_names, class_names, prefix + \"\\t\")\n",
    "\n",
    "print_tree(model.root, iris.feature_names, iris.target_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tree = DecisionTreeClassifier(criterion='entropy', max_depth=self.max_depth)\n",
    "        self.tree.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.tree.predict(X)\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the decision tree model\n",
    "model = DecisionTree(max_depth=3)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy and confusion matrix\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_mat)\n",
    "\n",
    "# Visualize the decision tree\n",
    "plt.figure(figsize=(12, 8))\n",
    "plot_tree(model.tree, filled=True, feature_names=iris.feature_names, class_names=iris.target_names)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
