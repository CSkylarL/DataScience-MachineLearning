{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "\n",
    "Random forest is an ensemble learning method that constructs multiple decision trees and combines their predictions to improve the overall accuracy and robustness of the model.\n",
    "\n",
    "## History\n",
    "\n",
    "Random forests were first introduced by Leo Breiman in 2001. They build upon the concept of bagging (Bootstrap Aggregating), introduced by Breiman in 1994, and decision trees, which have been a popular machine learning method since the 1980s.\n",
    "\n",
    "## Mathematical Equations\n",
    "\n",
    "Random forest does not have a specific mathematical equation like some other machine learning algorithms. Instead, it relies on the construction of multiple decision trees and combines their predictions through majority voting (for classification) or averaging (for regression).\n",
    "\n",
    "## Learning Algorithm\n",
    "\n",
    "The learning algorithm for random forest consists of the following steps:\n",
    "\n",
    "1. Draw `n` bootstrap samples from the original dataset, with replacement.\n",
    "2. For each bootstrap sample, grow a decision tree by recursively splitting nodes, with the following modifications:\n",
    "   - At each node, randomly select `m` features without replacement.\n",
    "   - Find the best split based on the selected features.\n",
    "3. Aggregate the predictions of the individual trees through majority voting (classification) or averaging (regression).\n",
    "\n",
    "The number of trees `n` and the number of features `m` to consider at each split are hyperparameters of the random forest algorithm.\n",
    "\n",
    "## Pros and Cons\n",
    "\n",
    "**Pros:**\n",
    "- Robust to overfitting due to the averaging of multiple trees.\n",
    "- Can handle a large number of features and high-dimensional data.\n",
    "- Works well with both categorical and continuous variables.\n",
    "- Can be easily parallelized, leading to faster training and prediction times.\n",
    "- Provides estimates of feature importance.\n",
    "\n",
    "**Cons:**\n",
    "- Less interpretable than single decision trees.\n",
    "- Slower to train and predict compared to simpler models, such as linear or logistic regression.\n",
    "- May not perform well with very sparse or high-dimensional data, such as text data in natural language processing tasks.\n",
    "\n",
    "## Suitable Tasks and Datasets\n",
    "\n",
    "Random forests can be applied to a wide range of tasks, including:\n",
    "\n",
    "- Classification\n",
    "- Regression\n",
    "- Feature selection\n",
    "- Anomaly detection\n",
    "\n",
    "They work well with datasets that have a mix of continuous and categorical variables, and they can handle missing data more effectively than some other algorithms.\n",
    "\n",
    "## Difference between Random Forest and Ensemble Decision Trees\n",
    "\n",
    "The main difference between random forests and ensemble decision trees is the way the individual trees are constructed and combined. In a random forest, each tree is grown using a bootstrap sample of the dataset, and at each node, only a random subset of features is considered for splitting. This introduces additional randomness and diversity among the trees, which helps to reduce overfitting and improve the overall accuracy of the model. In an ensemble of decision trees, the trees may be constructed using different methods, such as bagging or boosting, but they do not use the random feature selection strategy employed in random forests.\n",
    "\n",
    "## References\n",
    "\n",
    "1. Breiman, L. (2001). Random forests. Machine learning, 45(1), 5-32.\n",
    "2. Breiman, L. (1996). Bagging predictors. Machine Learning, 24(2), 123-140.\n",
    "3. Liaw, A., & Wiener, M. (2002). Classification and regression by randomForest. R news, 2(3), 18-22.\n",
    "4. Cutler, D. R., Edwards Jr, T. C., Beard, K. H., Cutler, A., Hess, K. T., Gibson, J., & Law\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# The DecisionTree class from the previous question\n",
    "class DecisionTree:\n",
    "    # ...\n",
    "\n",
    "# Bootstrap sample function\n",
    "def bootstrap_sample(X, y):\n",
    "    n_samples = X.shape[0]\n",
    "    indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "    return X[indices], y[indices]\n",
    "\n",
    "# Random Forest class\n",
    "class RandomForest:\n",
    "    def __init__(self, n_trees=100, min_samples_split=2, max_depth=100, n_features=None):\n",
    "        self.n_trees = n_trees\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.n_features = n_features\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.trees = []\n",
    "        for _ in range(self.n_trees):\n",
    "            tree = DecisionTree(min_samples_split=self.min_samples_split,\n",
    "                                max_depth=self.max_depth, n_features=self.n_features)\n",
    "            X_sample, y_sample = bootstrap_sample(X, y)\n",
    "            tree.fit(X_sample, y_sample)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        tree_preds = np.array([tree.predict(X) for tree in self.trees])\n",
    "        return np.array([Counter(col).most_common(1)[0][0] for col in tree_preds.T])\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the random forest model\n",
    "model = RandomForest(n_trees=100, min_samples_split=2, max_depth=100, n_features=None)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the random forest model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy and confusion matrix\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_mat)\n",
    "\n",
    "# Visualize the feature importances\n",
    "importances = model.feature_importances_\n",
    "indices = np.argsort(importances)\n",
    "\n",
    "plt.barh(range(X.shape[1]), importances[indices], align='center')\n",
    "plt.yticks(range(X.shape[1]), [iris.feature_names[i] for i in indices])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Feature Importances in Random Forest')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
