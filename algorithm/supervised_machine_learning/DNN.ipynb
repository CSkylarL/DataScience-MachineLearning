{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense Neural Network (DNN)\n",
    "##### Author: Chenyang Skylar Li\n",
    "\n",
    "# Table of Contents\n",
    "1. [Introduction](#introduction)\n",
    "2. [Mathematical Fundations](#mathematical-fundations)\n",
    "    - [Classification and Regression Trees (CART)](#classification-and-regression-trees-cart)\n",
    "3. [Learning Algorithm](#learning-algorithm)\n",
    "    - [Use DecisionTreeClassifier() class provided by Scikit-Learn in wine dataset](#use-decisiontreeclassifier-class-provided-by-scikit-learn-in-wine-dataset)\n",
    "    - [Use DecisionTreeRegressor() class provided by Scikit-Learn in california_housing dataset](#use-decisiontreeregressor-class-provided-by-scikit-learn-in-california_housing-dataset)\n",
    "4. [Pros and Cons](#pros-and-cons)\n",
    "5. [Suitable Tasks and Datasets](#suitable-tasks-and-datasets)\n",
    "6. [References](#references)\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Dense Neural Networks, also known as fully connected networks, are the simplest kind of artificial neural network. A neuron in a dense layer has a connection to every neuron in the previous layer. The concept of artificial neural networks dates back to the 1940s with the development of the perceptron model. Over the years, the technology has evolved with the advent of multi-layer perceptrons and deep learning.\n",
    "\n",
    "## Mathematical Foundations\n",
    "\n",
    "A Dense Neural Network (DNN) is a type of artificial neural network where each neuron in a layer is connected to all neurons in the previous and next layers. [Perceptron](perceptron.ipynb), [Linear Regression](./linear_regression.ipynb), and [Logistic Regression](./logistic_regression.ipynb), can be thought of as special cases of a neural \"network\", a single neuron with an appropriate activation function (or lack thereof in the case of Linear Regression). The power of a DNN comes from adding more layers of neurons and more neurons per layer, which allows the network to learn and represent more complex functions.\n",
    "\n",
    "\n",
    "In the dense neural network, a neuron in a dense layer takes the weighted sum of all its inputs, adds a bias term and then applies an activation function.\n",
    "\n",
    "Mathematically, for a given layer $l$, the output $h^{(l)}$ of that layer is given by:\n",
    "\n",
    "$$h^{(l)} = \\sigma(W^{(l)}h^{(l-1)} + b^{(l)})$$\n",
    "\n",
    "Where,\n",
    "- $W^{(l)}$ and $b^{(l)}$ are the weights and biases for layer $l$\n",
    "- $h^{(l-1)}$ is the output from the previous layer\n",
    "- $\\sigma$ is the activation function, including:\n",
    "    - Linear: $\\sigma(z) = z$, which is used in Linear Regression\n",
    "    - Step: $\\sigma(z) = \\begin{cases} 1 & z > 0 \\\\ 0 & z \\leq 0 \\end{cases}$, which is used in Perceptron\n",
    "    - Sign: $\\sigma(z) = \\begin{cases} 1 & z > 0 \\\\ -1 & z \\leq 0 \\end{cases}$, which is used in Perceptron\n",
    "    - Sigmoid: $\\sigma(z) = \\frac{1}{1 + e^{-z}}$, which is used in Logistic Regression\n",
    "    - Tanh: $\\sigma(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$, which is used in Logistic Regression\n",
    "    - Rectified Linear Unit (ReLU): $\\sigma(z) = \\max(0, z)$, which is used in DNNs\n",
    "    - Leaky ReLU: $\\sigma(z) = \\max(0.01z, z)$, which is used in DNNs\n",
    "    - Softmax: $\\sigma(z)_i = \\frac{e^{z_i}}{\\sum_{j=1}^n e^{z_j}}$, which is used in DNNs for multi-class classification\n",
    "\n",
    "## Learning Algorithm\n",
    "\n",
    "DNNs typically use a variant of [gradient descent](../optimization_algorithm/gradient_descent.ipynb) and `backpropagation` for learning. The algorithm iteratively adjusts the network's weights and biases to minimize a loss function.\n",
    "\n",
    "Training a Dense Neural Network typically involves the following steps:\n",
    "\n",
    "1. **Initialization**: Initialize the weights and biases with small random numbers. This is to break the symmetry and ensure different neurons learn different things.\n",
    "\n",
    "2. **Forward Propagation**: For each training instance in the dataset, perform a forward pass through the network. This involves computing the weighted sums and activations of all neurons.\n",
    "\n",
    "   For a neuron in layer $l$ with input $x$ from the previous layer, weights $w$, and bias $b$, the output $a$ (activation) is calculated as:\n",
    "\n",
    "    $$z = w \\cdot x + b$$\n",
    "    $$a = \\sigma(z)$$\n",
    "\n",
    "   where $\\sigma$ is the activation function (like ReLU, sigmoid, tanh, etc.), and $z$ is the weighted sum.\n",
    "\n",
    "3. **Loss Calculation**: At the output layer, calculate the loss (or cost) for the prediction. The loss function used depends on the task at hand (e.g., mean squared error for regression tasks, cross-entropy for binary classification tasks, etc.).\n",
    "\n",
    "4. **Backward Propagation (Backpropagation)**: Compute the gradient of the loss function with respect to each weight and bias in the network. This involves applying the chain rule to find these derivatives (gradients) from the output layer back to the input layer.\n",
    "\n",
    "   The gradients for layer $l$ are calculated as:\n",
    "\n",
    "    $$\\delta^{(l)} = ((w^{(l+1)})^T \\delta^{(l+1)}) \\odot \\sigma'(z^{(l)})$$\n",
    "    $$\\frac{\\partial C}{\\partial b^{(l)}_j} = \\delta^{(l)}_j$$\n",
    "    $$\\frac{\\partial C}{\\partial w^{(l)}_{jk}} = a^{(l-1)}_k \\delta^{(l)}_j$$\n",
    "    \n",
    "    Where:\n",
    "    - $j$ is the index of a neuron in layer $l$.\n",
    "    - $k$ is the index of a neuron in the previous layer ($l-1$).\n",
    "    - $w^{(l)}{jk}$ connecting the $k^{th}$ neuron in the $(l-1)^{th}$ layer to the $j^{th}$ neuron in the $l^{th}$ layer.\n",
    "    - $\\frac{\\partial C}{\\partial w^{(l)}{jk}}$ represents the rate of change of the cost function $C$ with respect to the weight.\n",
    "    - $a^{(l-1)}_k$ is the activation of the $k^{th}$ neuron in the $(l-1)^{th}$ layer.\n",
    "    - $\\delta^{(l)}_j$ is the error term associated with the $j^{th}$ neuron in the $l^{th}$ layer.\n",
    "    \n",
    "5. **Update Weights and Biases**: Update the weights and biases in the direction that decreases the loss. This is often done using a variant of Stochastic Gradient Descent (SGD). The weights $w$ and biases $b$ are updated as follows:\n",
    "\n",
    "    $$w = w - \\eta \\frac{\\partial C}{\\partial w}$$\n",
    "    $$b = b - \\eta \\frac{\\partial C}{\\partial b}$$\n",
    "\n",
    "   where $\\eta$ is the learning rate, a hyperparameter that controls how much we adjust the weights with respect to the loss gradient.\n",
    "\n",
    "6. **Iteration**: Repeat steps 2-5 for a set number of iterations or until the network's predictions are satisfactory. This is usually done over multiple epochs, where an epoch is one complete pass through the entire training dataset.\n",
    "\n",
    "\n",
    "\n",
    "## Pros and Cons\n",
    "\n",
    "**Pros:**\n",
    "- Can model complex, non-linear relationships\n",
    "- Can handle high-dimensional data\n",
    "- Scalable to large datasets\n",
    "\n",
    "**Cons:**\n",
    "- Requires a large amount of data\n",
    "- Prone to overfitting\n",
    "- Difficult to interpret\n",
    "\n",
    "## Suitable Tasks and Datasets\n",
    "\n",
    "## Suitable Tasks and Datasets for Dense Neural Networks\n",
    "\n",
    "DNNs are versatile and can handle a variety of tasks and datasets. Here are some examples:\n",
    "\n",
    "1. **Binary and Multiclass Classification**: Dense Neural Networks can handle binary and multiclass classification tasks quite effectively. For instance, they can be used for digit recognition, image classification, sentiment analysis, etc.\n",
    "\n",
    "2. **Regression**: They are also suitable for regression tasks where the goal is to predict a continuous output. For example, predicting house prices, stock prices, etc.\n",
    "\n",
    "3. **Feature Learning**: Dense Neural Networks can learn to extract useful features from raw data. This makes them suitable for tasks where handcrafted features are difficult to create.\n",
    "\n",
    "4. **Large Datasets**: Dense Neural Networks tend to perform better on larger datasets because they have the capacity to learn complex representations.\n",
    "\n",
    "5. **High-Dimensional Data**: Dense Neural Networks are capable of handling high-dimensional data, such as images, audio, and text. They can learn hierarchical representations of the data, which helps in capturing complex patterns.\n",
    "\n",
    "6. **Text and Time-Series Data**: Although Recurrent Neural Networks (RNNs) or Transformers are often more suitable for sequence data, Dense Neural Networks can still be used with some success, especially when sequence length is fixed and relatively short.\n",
    "\n",
    "It is important to note that the performance of a Dense Neural Network is dependent on the quality and the quantity of the data, the network architecture, and the choice of hyperparameters.\n",
    "\n",
    "## References\n",
    "1. Goodfellow, I., Bengio, Y., & Courville, A. (2016). [Deep learning](http://www.deeplearningbook.org/). MIT press.\n",
    "2. Keras Documentation: [Getting started with the Keras Sequential model](https://keras.io/getting-started/sequential-model-guide/)\n",
    "3. Keras Documentation: [Guide to the Sequential model](https://keras.io/models/sequential/)\n",
    "4. TensorFlow Documentation: [tf.keras.Sequential](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential)\n",
    "5. GÃ©ron, A. (2022). Hands-on machine learning with Scikit-Learn, Keras, and TensorFlow. \" O'Reilly Media, Inc.\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mimport\u001b[39;00m keras\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m layers\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Load the Fashion MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "# Scale the pixel values to be between 0 and 1\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# Create a dense neural network with 3 hidden layers\n",
    "model = keras.Sequential([\n",
    "    layers.Flatten(input_shape=(28, 28)),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print('Test accuracy:', test_acc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
