{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means Clustering Algorithm\n",
    "\n",
    "K-means is a popular partitioning-based clustering algorithm that aims to group similar data points into clusters by minimizing the within-cluster sum of squares.\n",
    "\n",
    "## History\n",
    "\n",
    "The k-means algorithm was first proposed by Stuart Lloyd in 1957, but the work was not published until 1982. In 1965, Edward W. Forgy published a similar method, which became known as the k-means algorithm.\n",
    "\n",
    "## Mathematical Equations\n",
    "\n",
    "The objective of k-means is to minimize the within-cluster sum of squares (WCSS), defined as:\n",
    "\n",
    "WCSS = Σ(Σ(||x - μ_i||^2))\n",
    "\n",
    "where x is a data point, μ_i is the centroid of cluster i, and ||.|| denotes the Euclidean distance.\n",
    "\n",
    "## Learning Algorithm\n",
    "\n",
    "The learning algorithm for k-means consists of the following steps:\n",
    "\n",
    "1. Initialize the centroids randomly by selecting k data points from the dataset.\n",
    "2. Assign each data point to the nearest centroid.\n",
    "3. Update the centroids by calculating the mean of all the data points assigned to each centroid.\n",
    "4. Repeat steps 2 and 3 until the centroids' positions do not change significantly or a maximum number of iterations is reached.\n",
    "\n",
    "The number of clusters (k) is an input parameter of the k-means algorithm.\n",
    "\n",
    "## Pros and Cons\n",
    "\n",
    "**Pros:**\n",
    "- Easy to understand and implement.\n",
    "- Efficient in terms of time complexity.\n",
    "- Works well with large datasets.\n",
    "- Guaranteed to converge.\n",
    "\n",
    "**Cons:**\n",
    "- Requires the number of clusters as an input parameter.\n",
    "- Sensitive to the initial placement of centroids.\n",
    "- Assumes that clusters are spherical and have similar densities.\n",
    "- Can get stuck in local optima.\n",
    "- Does not work well with categorical data.\n",
    "\n",
    "## Suitable Tasks and Datasets\n",
    "\n",
    "K-means can be applied to a variety of clustering tasks, including:\n",
    "\n",
    "- Image segmentation\n",
    "- Anomaly detection\n",
    "- Market segmentation\n",
    "- Document clustering\n",
    "\n",
    "It works well with datasets that have spherical clusters and similar densities. K-means is not suitable for datasets with arbitrary shapes or varying densities.\n",
    "\n",
    "## Difference between K-means and DBSCAN\n",
    "\n",
    "The main differences between k-means and DBSCAN are:\n",
    "\n",
    "- K-means requires the number of clusters as an input parameter, while DBSCAN does not.\n",
    "- K-means is sensitive to the initial placement of centroids and may converge to local optima, while DBSCAN is more robust due to its density-based approach.\n",
    "- K-means tends to work well with spherical clusters and may struggle with clusters of arbitrary shapes, while DBSCAN can find clusters of any shape.\n",
    "- K-means is less robust to noise compared to DBSCAN, which can identify and separate noise points from clusters.\n",
    "- DBSCAN can handle datasets with varying densities, while k-means assumes similar densities across clusters.\n",
    "\n",
    "## References\n",
    "\n",
    "1. Lloyd, S. P. (1982). Least squares quantization in PCM. IEEE Transactions on Information Theory, 28(2), 129-137.\n",
    "2. Forgy, E. W. (1965). Cluster analysis of multivariate data: efficiency vs interpretability of classifications. Biometrics, 21(3), 768-769.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Function to calculate Euclidean distance between two points\n",
    "def euclidean_distance(a, b):\n",
    "    return np.sqrt(np.sum((a - b) ** 2))\n",
    "\n",
    "# K-means class\n",
    "class KMeans:\n",
    "    def __init__(self, n_clusters=3, max_iter=300, random_state=None):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.max_iter = max_iter\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit_predict(self, X):\n",
    "        np.random.seed(self.random_state)\n",
    "        centroids = X[np.random.choice(X.shape[0], self.n_clusters, replace=False)]\n",
    "\n",
    "        for _ in range(self.max_iter):\n",
    "            clusters = [np.argmin([euclidean_distance(x, centroid) for centroid in centroids]) for x in X]\n",
    "            new_centroids = [X[np.array(clusters) == i].mean(axis=0) for i in range(self.n_clusters)]\n",
    "\n",
    "            if np.allclose(centroids, new_centroids):\n",
    "                break\n",
    "            centroids = new_centroids\n",
    "\n",
    "        self.cluster_centers_ = np.array(centroids)\n",
    "        return np.array(clusters)\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data[:, :2]  # Use only the first two features for easy visualization\n",
    "\n",
    "# Apply the k-means algorithm\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "clusters = kmeans.fit_predict(X)\n",
    "\n",
    "# Visualize the results\n",
    "plt.scatter(X[:, 0], X[:, 1], c=clusters, cmap='viridis')\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='red', marker='x')\n",
    "plt.xlabel('Sepal Length')\n",
    "plt.ylabel('Sepal Width')\n",
    "plt.title('K-means Clustering')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data[:, :2]  # Use only the first two features for easy visualization\n",
    "\n",
    "# Apply the k-means algorithm\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "clusters = kmeans.fit_predict(X)\n",
    "\n",
    "# Evaluate the model using the elbow method\n",
    "inertia_values = []\n",
    "K = range(1, 11)\n",
    "for k in K:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X)\n",
    "    inertia_values.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot(K, inertia_values, 'bo-')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('The Elbow Method')\n",
    "plt.show()\n",
    "\n",
    "# Visualize the results\n",
    "plt.scatter(X[:, 0], X[:, 1], c=clusters, cmap='viridis')\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='red', marker='x')\n",
    "plt.xlabel('Sepal Length')\n",
    "plt.ylabel('Sepal Width')\n",
    "plt.title('K-means Clustering')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
