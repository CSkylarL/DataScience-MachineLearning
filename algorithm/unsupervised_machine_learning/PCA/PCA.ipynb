{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA)\n",
    "\n",
    "Principal Component Analysis (PCA) is a dimensionality reduction technique that is widely used in machine learning for data preprocessing, visualization, and noise reduction. PCA aims to project high-dimensional data onto a lower-dimensional space while preserving as much of the variance in the data as possible.\n",
    "\n",
    "## History\n",
    "\n",
    "PCA was first introduced by Karl Pearson in 1901 as a method for transforming correlated variables into linearly uncorrelated variables. Later in 1933, Harold Hotelling extended PCA to the analysis of multivariate data and provided a solid mathematical foundation for the method.\n",
    "\n",
    "## Mathematical Equations\n",
    "\n",
    "PCA involves the following steps:\n",
    "\n",
    "1. Calculate the covariance matrix of the dataset.\n",
    "2. Compute the eigenvalues and eigenvectors of the covariance matrix.\n",
    "3. Sort the eigenvalues in descending order and select the top k eigenvectors corresponding to the k largest eigenvalues.\n",
    "4. Project the original data onto the lower-dimensional space spanned by the top k eigenvectors.\n",
    "\n",
    "The covariance matrix (Σ) of a dataset with n features is an n x n symmetric matrix, where the element at the ith row and jth column is the covariance between the ith and jth features:\n",
    "\n",
    "Σ_ij = Cov(X_i, X_j)\n",
    "\n",
    "The eigenvalues (λ) and eigenvectors (v) of the covariance matrix satisfy the following equation:\n",
    "\n",
    "Σv = λv\n",
    "\n",
    "## Learning Algorithm\n",
    "\n",
    "The learning algorithm for PCA consists of the following steps:\n",
    "\n",
    "1. Standardize the dataset (mean = 0, standard deviation = 1) to ensure equal importance of all features.\n",
    "2. Calculate the covariance matrix of the standardized dataset.\n",
    "3. Compute the eigenvalues and eigenvectors of the covariance matrix.\n",
    "4. Sort the eigenvalues in descending order and select the top k eigenvectors.\n",
    "5. Project the original data onto the lower-dimensional space spanned by the top k eigenvectors.\n",
    "\n",
    "## Pros and Cons\n",
    "\n",
    "**Pros:**\n",
    "- Reduces the dimensionality of the data, which can help overcome the curse of dimensionality and improve the performance of machine learning algorithms.\n",
    "- Can help visualize high-dimensional data.\n",
    "- Removes multicollinearity between features and improves interpretability of the results.\n",
    "- Can be used for noise reduction in the data.\n",
    "\n",
    "**Cons:**\n",
    "- Assumes that the principal components are linear combinations of the original features.\n",
    "- Loss of information due to the reduction in dimensionality.\n",
    "- Sensitive to the scaling of the features.\n",
    "\n",
    "## Suitable Tasks and Datasets\n",
    "\n",
    "PCA can be applied to various tasks, including:\n",
    "\n",
    "- Data preprocessing\n",
    "- Visualization of high-dimensional data\n",
    "- Noise reduction\n",
    "- Feature extraction\n",
    "\n",
    "It is suitable for datasets with continuous features and can be particularly helpful when the dataset has a large number of features or when there is multicollinearity between the features.\n",
    "\n",
    "## References\n",
    "\n",
    "1. Pearson, K. (1901). On lines and planes of closest fit to systems of points in space. Philosophical Magazine, 2(11), 559-572.\n",
    "2. Hotelling, H. (1933). Analysis of a complex of statistical variables into principal components. Journal of Educational Psychology, 24(6), 417-441.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class PCA:\n",
    "    def __init__(self, n_components):\n",
    "        self.n_components = n_components\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        # Standardize the input dataset\n",
    "        X_standardized = StandardScaler().fit_transform(X)\n",
    "\n",
    "        # Calculate the covariance matrix\n",
    "        covariance_matrix = np.cov(X_standardized.T)\n",
    "\n",
    "        # Compute the eigenvalues and eigenvectors\n",
    "        eigenvalues, eigenvectors = np.linalg.eig(covariance_matrix)\n",
    "\n",
    "        # Sort the eigenvalues and eigenvectors in descending order\n",
    "        sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "        sorted_eigenvalues = eigenvalues[sorted_indices]\n",
    "        sorted_eigenvectors = eigenvectors[:, sorted_indices]\n",
    "\n",
    "        # Select the top n_components eigenvectors\n",
    "        top_eigenvectors = sorted_eigenvectors[:, :self.n_components]\n",
    "\n",
    "        # Project the data onto the lower-dimensional space\n",
    "        X_reduced = X_standardized.dot(top_eigenvectors)\n",
    "        return X_reduced\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Apply PCA to the dataset, reducing the dimensionality to 2\n",
    "pca = PCA(n_components=2)\n",
    "X_reduced = pca.fit_transform(X)\n",
    "\n",
    "# Visualize the results\n",
    "plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y, cmap='viridis')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('PCA on Iris Dataset')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
