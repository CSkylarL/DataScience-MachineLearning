{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "Gradient descent is an optimization algorithm used to find the minimum of a function. It is commonly used in machine learning and deep learning to minimize the loss function and update the model's parameters.\n",
    "\n",
    "## History\n",
    "\n",
    "Gradient descent was first proposed by Cauchy in 1847. It has since been widely used in optimization problems, particularly in the field of machine learning, where it has become one of the most popular algorithms for training models.\n",
    "\n",
    "## Mathematical Equations\n",
    "\n",
    "Given a function `f(w)`, where `w` is a vector of parameters, the gradient descent algorithm aims to find the minimum of the function. The gradient of the function (∇f(w)) represents the direction of the steepest increase in the function's value. The update rule for gradient descent is:\n",
    "\n",
    "w(t+1) = w(t) - η * ∇f(w(t))\n",
    "\n",
    "where:\n",
    "- w(t): The parameter vector at iteration t.\n",
    "- η: The learning rate, which controls the step size in each iteration.\n",
    "- ∇f(w(t)): The gradient of the function evaluated at w(t).\n",
    "\n",
    "The learning rate η is a hyperparameter that needs to be tuned. If it is too small, the algorithm will converge slowly, while if it is too large, it may overshoot the minimum and diverge.\n",
    "\n",
    "## Learning Algorithm\n",
    "\n",
    "The learning algorithm for gradient descent consists of iteratively updating the model's parameters by following the negative gradient of the loss function. The main steps are:\n",
    "\n",
    "1. Initialize the model's parameters randomly or using a predefined method.\n",
    "2. Calculate the gradient of the loss function with respect to each parameter.\n",
    "3. Update the parameters using the gradient descent update rule.\n",
    "4. Repeat steps 2 and 3 until a stopping criterion is met (e.g., maximum number of iterations, minimum change in the loss function, or minimum change in the parameters).\n",
    "\n",
    "## Pros and Cons\n",
    "\n",
    "**Pros:**\n",
    "- Simple and easy to understand.\n",
    "- Can be applied to a wide range of problems.\n",
    "- Efficient for large-scale datasets and problems with many parameters.\n",
    "- Can be adapted to use adaptive learning rates or momentum.\n",
    "\n",
    "**Cons:**\n",
    "- Sensitive to the choice of learning rate and other hyperparameters.\n",
    "- Can get stuck in local minima for non-convex functions.\n",
    "- May be slow to converge for ill-conditioned problems or problems with a high degree of curvature.\n",
    "- Requires the calculation of gradients, which can be computationally expensive for complex models.\n",
    "\n",
    "## Suitable Tasks and Datasets\n",
    "\n",
    "Gradient descent can be applied to a wide range of optimization tasks in machine learning and deep learning, including:\n",
    "\n",
    "- Linear regression\n",
    "- Logistic regression\n",
    "- Neural networks\n",
    "- Support vector machines (with the appropriate kernel and loss function)\n",
    "\n",
    "It is particularly useful for problems with large datasets or a high number of parameters, where other optimization algorithms may be too computationally expensive.\n",
    "\n",
    "## References\n",
    "\n",
    "1. Cauchy, A. (1847). Méthode générale pour la résolution des systèmes d'équations simultanées. Comptes Rendus, 25, 536-538.\n",
    "2. Bottou, L. (2010). Large-scale machine learning with stochastic gradient descent. In Proceedings of the 19th International Conference on Computational Statistics (pp. 177-186). Springer.\n",
    "3. Ruder, S. (2016). An overview of gradient descent optimization algorithms. arXiv preprint arXiv:1609.04747.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "class GradientDescent:\n",
    "    def __init__(self, learning_rate=0.01, max_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iterations = max_iterations\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Add a column of ones to X for the bias term\n",
    "        X = np.hstack((np.ones((n_samples, 1)), X))\n",
    "\n",
    "        # Initialize weights\n",
    "        self.weights = np.random.rand(n_features + 1)\n",
    "\n",
    "        # Perform gradient descent\n",
    "        for _ in range(self.max_iterations):\n",
    "            gradient = (2 / n_samples) * X.T.dot(X.dot(self.weights) - y)\n",
    "            self.weights -= self.learning_rate * gradient\n",
    "\n",
    "    def predict(self, X):\n",
    "        n_samples = X.shape[0]\n",
    "        X = np.hstack((np.ones((n_samples, 1)), X))\n",
    "        return X.dot(self.weights)\n",
    "\n",
    "# Generate a synthetic regression dataset\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=20, random_state=42)\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the linear regression model using gradient descent\n",
    "model = GradientDescent(learning_rate=0.1, max_iterations=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the mean squared error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "\n",
    "# Visualize the results\n",
    "plt.scatter(X_test, y_test, color='blue', label='Actual')\n",
    "plt.scatter(X_test, y_pred, color='red', label='Predicted')\n",
    "plt.legend()\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Linear Regression using Gradient Descent')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# This code defines a simple gradient descent class and applies it to linear regression. The learning rate and maximum number of iterations are hyperparameters that can be adjusted. In this example, we use a learning rate of 0.1 and 1000 iterations. The model is evaluated on a test set using mean squared error, and the results are visualized in a scatter plot.\n",
    "\n",
    "The learning rate is an important hyperparameter that affects the convergence of the gradient descent algorithm. If it is too small, the algorithm may converge slowly or get stuck in a local minimum. If it is too large, the algorithm may overshoot the minimum and diverge. Experimenting with different learning rates can help find the optimal value for a given problem."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
