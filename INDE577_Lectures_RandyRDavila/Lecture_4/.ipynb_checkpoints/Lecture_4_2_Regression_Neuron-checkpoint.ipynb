{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Lecture 4.2 The Single Neuron Linear Regression Model\n",
    "\n",
    "In this notebook we implement the single neuron model together with the gradient descent algorithm in order to solve the **linear regression problem**. For teaching purposes, this notebook will focus on single variable regression for a single species of flower in the readily available iris dataset.  \n",
    "\n",
    "## 4.2.1 Regression\n",
    "Let $\\mathcal{X}$ be the space of all possible feature vectors, let $\\mathcal{Y}$ be the space of all possible corresponding labels for the feature vectors, and let $f:\\mathcal{X} \\rightarrow \\mathcal{Y}$ be the optimal target function assigning labels to feature vectors in $\\mathcal{Y}$. Next recall that in supervised machine learning we observe some subset of features and labels as shown in the figure below. \n",
    "\n",
    "Supervised Machine Learning\n",
    "---\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"supervised_learning.png\" width=\"500\">\n",
    "</p>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "In [regression](https://favtutor.com/blogs/types-of-regression), machine learning models are given labeled data $\\mathcal{D} = \\{(\\mathbf{x}^1, y^1), \\dots, (\\mathbf{x}^N, y^N)\\}$, where the feature vectors satisfy $\\mathbf{x}^{(i)} \\in \\mathbb{R}$ and the target labels satify $y^{(i)} \\in \\mathbb{R}$. Thus, this supervised learning task seeks to predict real valued target labels. This is different from classification (such as the perceptron single neuron model) as the following figure suggests.\n",
    "\n",
    "Regression versus Classification\n",
    "---\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"Regression_VS_Classification.png\" width=\"600\" height =\"800\">\n",
    "</p>\n",
    "\n",
    "---\n",
    "\n",
    "### - Linear Regression\n",
    "\n",
    "In this notebook we will focus on **linear regression**. This specific case of regression assumes that the *target values in $\\mathcal{Y}$ are approximated by a linear function of the associated feature vectors*. That is, the optimal target function $f:\\mathcal{X} \\rightarrow \\mathcal{Y}$ is assumed the be roughly a linear function. \n",
    "\n",
    "Many types of problems and data fall into this roughly linear relationship. For example, one could reasonably suspect that such a relationship exists when considering the iris dataset setosa species sepal length as feature measurements together with the iris dataset setosa species sepal width as targets. This can be verified by visualizing the data. Examine the figure generated by running the following code in the cell below.  \n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "\n",
    "df = pd.read_csv(\"Datasets/iris_dataset.csv\")\n",
    "df = df.iloc[:50][[\"sepal_length\", \"sepal_width\"]]\n",
    "plt.figure(figsize = (10, 8))\n",
    "plt.scatter(df.sepal_length, \n",
    "            df.sepal_width, \n",
    "            color = \"lightseagreen\",\n",
    "            label = \"setosa flowers\")\n",
    "plt.xlabel(\"sepal length [cm]\", fontsize = 15)\n",
    "plt.ylabel(\"sepal width [cm]\", fontsize = 15)\n",
    "plt.legend(fontsize = 15)\n",
    "plt.title(\"My Regression Data\", fontsize = 18)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "\n",
    "df = pd.read_csv(\"Datasets/iris_dataset.csv\")\n",
    "df = df.iloc[:50][[\"sepal_length\", \"sepal_width\"]]\n",
    "plt.figure(figsize = (10, 7))\n",
    "plt.scatter(df.sepal_length, \n",
    "            df.sepal_width, \n",
    "            color = \"lightseagreen\",\n",
    "            label = \"setosa flowers\")\n",
    "plt.xlabel(\"sepal length [cm]\", fontsize = 15)\n",
    "plt.ylabel(\"sepal width [cm]\", fontsize = 15)\n",
    "plt.legend(fontsize = 15)\n",
    "plt.title(\"My Regression Data\", fontsize = 18)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4.2.2 Linear Regression Single Neuron Model\n",
    "\n",
    "As discussed in a [previous lecture](https://www.youtube.com/watch?v=SmEKxsd_67w&t=1s) there exists a general machine learning model for supervised learning. In this model the optimal target function $f:\\mathcal{X} \\rightarrow \\mathcal{Y}$ assigns the correct labels to every possible feature measurement. Next recall that our goal is to find a reasonable hypthesis $h:\\mathcal{X} \\rightarrow \\mathcal{Y}$, which approximates the target function $f$. \n",
    "\n",
    "---\n",
    "\n",
    "General ML Model:\n",
    "---\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"General_ML_Model.png\" width=\"500\">\n",
    "</p>\n",
    "\n",
    "---\n",
    "\n",
    "Because we are assuming the target function $f:\\mathcal{X} \\rightarrow \\mathcal{Y}$ is a **linear function of the input features**, and because we know single neuron models are good function approximators, we next build a single neuron model with a *linear-activation* activation function. Furthermore, in this model we choose the *mean-sqaured error* cost function:\n",
    "\n",
    "$$\n",
    "C(\\mathbf{w}, b) = \\frac{1}{2N}\\sum_{i=1}^{N}\\Big(\\hat{y}^{(i)} - y^{(i)}\\Big)^2. \n",
    "$$\n",
    "\n",
    "With our specific case of linear regression on the setosa iris dataset with a single feature measurment taken from sepal length data, we next construct a single neuron model with a linear activation function and the mean-sqaured error cost function as depicted in the figure below.\n",
    "\n",
    "---\n",
    "\n",
    "Single Neuron Linear Regression Model\n",
    "---\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"regression_neuron.png\" width=\"500\">\n",
    "</p>\n",
    "\n",
    "---\n",
    "\n",
    "## 4.2.3 Minimize the Cost Function $C(w_1, b)$\n",
    "\n",
    "Before defining a custom ```SingleNeuron``` class, we first need first discuss how to minimize the neurons cost function. More specifically, we wish to solve the following optimization problem:\n",
    "\n",
    "$$\n",
    "\\min_{w_1, b}C(w_1, b)\n",
    "$$\n",
    "\n",
    "Since $C(w_1, b)$ is a differentiable function of both $w_1$ and $b$, we may attempt to solve this minimization problem by applying the gradient descent algorithm:\n",
    "\n",
    "$$\n",
    "w_1 \\leftarrow w_1 - \\alpha \\frac{\\partial C}{\\partial w_1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b \\leftarrow b - \\alpha \\frac{\\partial C}{\\partial b}\n",
    "$$\n",
    "\n",
    "### - Finding the Partial Derivatives of $C(w_1, b)$ \n",
    "In order to implement the gradient descent method we first need to understand how the partial derivatives of $C(w_1, b)$ are calculated over the training data at hand. With this in mind, suppose for now that we are calculating the mean-sqaured error cost function on a *single example* example of data, i.e., $N = 1$. For this single example we observe that the mean-sqaured error cost function becomes: \n",
    "\n",
    "$$\n",
    "C(w, b; \\mathbf{x}^{(i)}, y^{(i)}) = \\frac{1}{2}\\Big(\\hat{y}^{(i)} - y^{(i)}\\Big)^2. \n",
    "$$\n",
    "\n",
    "In the case of a linear activation function, it is important to note that $\\hat{y}^{(i)}$ is a very simple function of both $w_1$ and $b$. More specifically, we observe:\n",
    "\n",
    "$$\n",
    "\\hat{y}^{(i)} = a = z = w_1x^{(i)} + b. \n",
    "$$\n",
    "\n",
    "Thus, we may rewrite our neuron cost function with a single observation:\n",
    "\n",
    "$$\n",
    "C(w, b; \\mathbf{x}^{(i)}, y^{(i)}) = \\frac{1}{2}\\Big(w_1x^{(i)} + b - y^{(i)}\\Big)^2. \n",
    "$$\n",
    "\n",
    "With this equation, we can calculate $\\partial C/ \\partial w_1$ and $\\partial C/ \\partial b$ easily by applying the [chain rule (click for a quick refresher on the concept)](https://www.youtube.com/watch?v=HaHsqDjWMLU). The resulting partial derivatives with respect to $w_1$ and $b$ shown by the following equations:\n",
    "\n",
    "1. $\\frac{\\partial C(w_1, b; \\mathbf{x}^{(i)}, y^{(i)})}{\\partial w_1} = (w_1x^{(i)} + b - y^{(i)})x^{(i)} = (\\hat{y}^{(i)} - y^{(i)})x^{(i)}$\n",
    "2. $\\frac{\\partial C(w_1, b; \\mathbf{x}^{(i)}, y^{(i)})}{\\partial b} = (w_1x^{(i)} + b - y^{(i)}) = (\\hat{y}^{(i)} - y^{(i)})$\n",
    "\n",
    "Understanding the different ways in which we may calculate the partial derivatives of our cost function is essential in applying any *first-order* minimization technique on the cost function $C(w_1, b)$. With what follows we discuss two of the three fundamental methods used to accomplish this goal. \n",
    "\n",
    "\n",
    "### - Different Flavors of First-Order Minimization \n",
    "When considering a single instance of data, we easily calculated $\\frac{\\partial C}{\\partial w_1}$ and $\\frac{\\partial C}{\\partial b}$ by applying the chain-rule. This notion can now be extended to all data used in training by summing the gradients calculated at entry of data. We will refer to this process as calculating the **full gradient** (or **full partial derivatives**) with respect to the training data: \n",
    "\n",
    "1. $\\frac{\\partial C(w_1, b; \\mathbf{X}, y)}{\\partial w_1} = \\frac{1}{N}\\sum_{i=1}^{N}\\Big(\\hat{y}^{(i)} - y^{(i)}\\Big)x^{(i)}$\n",
    "2. $\\frac{\\partial C(w_1, b; \\mathbf{X}, y)}{\\partial b} = \\frac{1}{N}\\sum_{i=1}^{N}\\Big(\\hat{y}^{(i)} - y^{(i)}\\Big)$\n",
    "\n",
    "Calculating the full gradient with respect to all training data and applying the gradient descent algorithm is called **batch gradient descent**.\n",
    "\n",
    "**Flavor 1. Batch Gradient Descent Algorithm:**\n",
    "1. For each epoch **do**\n",
    "2. Calculate the full gradient by finding $\\frac{\\partial C(w_1, b; \\mathbf{X}, y)}{\\partial w_1}$ and $\\frac{\\partial C(w_1, b; \\mathbf{X}, y)}{\\partial b}$.\n",
    "3. $w \\leftarrow w - \\alpha \\frac{\\partial C(w_1, b; \\mathbf{X}, y)}{\\partial w_1}$\n",
    "4. $b \\leftarrow b - \\alpha \\frac{\\partial C(w_1, b; \\mathbf{X}, y)}{\\partial b}$\n",
    "\n",
    "Applying batch gradient descent will work. However, *this method can be very slow and use a lot of memory*, especially when the number of training data is very large (possibly millions). More importantly, **batch gradient descent is not necessary to find local minima**. \n",
    "\n",
    "The most common way work around for this problem is to update $w_1$ and $b$ by calculating the gradient with respect to one entry of data at a time. This technique is called **stochastic gradient descent** and is one of the primary tools in training deep neural networks and simple single neuron models.  \n",
    "\n",
    "**Flavor 2. Stochastic Gradient Descent Algorithm:**\n",
    "1. For each epoch **do**\n",
    "2. For $i = 1, \\dots, N$ **do**\n",
    "3. Calculate $\\frac{\\partial C(w_1, b; \\mathbf{x}^{(i)}, y^{(i)})}{\\partial w_1}$ and $\\frac{\\partial C(\\partial C(w_1, b; \\mathbf{x}^{(i)}, y^{(i)}))}{\\partial b}$.\n",
    "2. $w \\leftarrow w - \\alpha \\frac{\\partial C(w_1, b; \\mathbf{x}^{(i)}, y^{(i)})}{\\partial w_1}$\n",
    "3. $b \\leftarrow b - \\alpha \\frac{\\partial C(w_1, b; \\mathbf{x}^{(i)}, y^{(i)})}{\\partial b}$\n",
    "\n",
    "For single neuron models in practice, stochastic gradient descent should be the preferred way for optimizing the weights and bias by minimizing the cost function. We implement stochastic gradient descent with the ```train``` method used in the following custom ```SingleNeuron``` class. \n",
    "```python\n",
    "class SingleNeuron(object):\n",
    "    \"\"\"\n",
    "    A class used to represent a single artificial neuron. \n",
    "\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    activation_function : function\n",
    "        The activation function applied to the preactivation linear combination.\n",
    "        \n",
    "    w_ : numpy.ndarray\n",
    "        The weights and bias of the single neuron. The last entry being the bias. \n",
    "        This attribute is created when the train method is called.\n",
    "        \n",
    "    errors_: list\n",
    "        A list containing the mean sqaured error computed after each iteration \n",
    "        of stochastic gradient descent per epoch. \n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    train(self, X, y, alpha = 0.005, epochs = 50)\n",
    "        Iterates the stochastic gradient descent algorithm through each sample \n",
    "        a total of epochs number of times with learning rate alpha. The data \n",
    "        used consists of feature vectors X and associated labels y. \n",
    "        \n",
    "    predict(self, X)\n",
    "        Uses the weights and bias, the feature vectors in X, and the \n",
    "        activation_function to make a y_hat prediction on each feature vector. \n",
    "    \"\"\"\n",
    "    def __init__(self, activation_function):\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "    def train(self, X, y, alpha = 0.005, epochs = 50):\n",
    "        self.w_ = np.random.rand(1 + X.shape[1])\n",
    "        self.errors_ = []\n",
    "        N = X.shape[0]\n",
    "\n",
    "        for _ in range(epochs):\n",
    "            errors = 0\n",
    "            for xi, target in zip(X, y):\n",
    "                self.w_[:-1] -= alpha*(self.predict(xi) - target)*xi\n",
    "                self.w_[-1] -= alpha*(self.predict(xi) - target)\n",
    "                errors += .5*((self.predict(xi) - target)**2)\n",
    "            self.errors_.append(errors/N)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        preactivation = np.dot(X, self.w_[:-1]) + self.w_[-1]\n",
    "        return self.activation_function(preactivation)\n",
    "\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleNeuron(object):\n",
    "    \"\"\"\n",
    "    A class used to represent a single artificial neuron. \n",
    "\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    activation_function : function\n",
    "        The activation function applied to the preactivation linear combination.\n",
    "\n",
    "    w_ : numpy.ndarray\n",
    "        The weights and bias of the single neuron. The last entry being the bias. \n",
    "        This attribute is created when the train method is called.\n",
    "\n",
    "    errors_: list\n",
    "        A list containing the mean sqaured error computed after each iteration \n",
    "        of stochastic gradient descent per epoch. \n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    train(self, X, y, alpha = 0.005, epochs = 50)\n",
    "        Iterates the stochastic gradient descent algorithm through each sample \n",
    "        a total of epochs number of times with learning rate alpha. The data \n",
    "        used consists of feature vectors X and associated labels y. \n",
    "\n",
    "    predict(self, X)\n",
    "        Uses the weights and bias, the feature vectors in X, and the \n",
    "        activation_function to make a y_hat prediction on each feature vector. \n",
    "    \"\"\"\n",
    "    def __init__(self, activation_function):\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "    def train(self, X, y, alpha = 0.005, epochs = 50):\n",
    "        self.w_ = np.random.rand(1 + X.shape[1])\n",
    "        self.errors_ = []\n",
    "        N = X.shape[0]\n",
    "\n",
    "        for _ in range(epochs):\n",
    "            errors = 0\n",
    "            for xi, target in zip(X, y):\n",
    "                error = (self.predict(xi) - target)\n",
    "                self.w_[:-1] -= alpha*error*xi\n",
    "                self.w_[-1] -= alpha*error\n",
    "                errors += .5*(error**2)\n",
    "            self.errors_.append(errors/N)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        preactivation = np.dot(X, self.w_[:-1]) + self.w_[-1]\n",
    "        return self.activation_function(preactivation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### - Quick Data Formatting\n",
    "\n",
    "Now that we have defined our custom ```SingleNeuron``` class, we next prep our data for training. By running the following code in the cell below, note that the first two lines of code convert the setosa sepal length column of data into a ```numpy.ndarray```. \n",
    "\n",
    "**CAUTION:** Single entry feature vectors need to be reshaped using the ```reshape(-1, 1)``` method. This is needed because of the dimensions required for vector and matrix multiplications.\n",
    "\n",
    "The third line converts the setosa sepal width column of data to a ```numpy.ndarray```. \n",
    "```python\n",
    "X = df.sepal_length.values\n",
    "X = X.reshape(-1, 1)\n",
    "y = df.sepal_width.values\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.sepal_length.values\n",
    "X = X.reshape(-1, 1)\n",
    "y = df.sepal_width.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "We can double check that we have formated the data correctly by scatter plotting the data again. Run the following code in the cell below. \n",
    "```python\n",
    "plt.figure(figsize = (10, 8))\n",
    "plt.scatter(X, y, color = \"lightseagreen\", label = \"setosa flowers\")\n",
    "plt.xlabel(\"sepal length [cm]\", fontsize = 15)\n",
    "plt.ylabel(\"sepal width [cm]\", fontsize = 15)\n",
    "plt.title(\"Setosa Regression Data\", fontsize = 18)\n",
    "plt.legend(fontsize = 15)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 8))\n",
    "plt.scatter(X, y, color = \"lightseagreen\", label = \"setosa flowers\")\n",
    "plt.xlabel(\"sepal length [cm]\", fontsize = 15)\n",
    "plt.ylabel(\"sepal width [cm]\", fontsize = 15)\n",
    "plt.title(\"Setosa Regression Data\", fontsize = 18)\n",
    "plt.legend(fontsize = 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### - Creating and Training an Instance of the ```SingleNeuron``` Class\n",
    "\n",
    "In order to instantiate a given instance the```SingleNeuron``` model, we need to first define an activation function. After doing so, we can then instantiate a ```SingleNeuron``` object. After creating this ```SingleNeuron```, we can then train it by calling the ```train()``` method with input ```X``` and ```y```. For demonstration purposes, we also pass the keyword arguments ```alpha = 0.0001``` and ```epochs = 5``` into the ```train()``` method. Do this by running the following code in the cell below. \n",
    "```python\n",
    "def linear_activation(z):\n",
    "    return z\n",
    "\n",
    "node = SingleNeuron(linear_activation)\n",
    "node.train(X, y, alpha = 0.0001, epochs = 5)\n",
    "```\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation(z):\n",
    "    return z\n",
    "\n",
    "node = SingleNeuron(linear_activation)\n",
    "node.train(X, y, alpha = 0.0001, epochs = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Now that we have created an instance of the ```SingleNeuron``` class and called the train method, we can visualize the linear regression line by scatter plotting the data and also ploting the predicted output over some domain within the range of values of input features. Do this by running the following code in the cell below (don't forget that since we are dealing with single measurement features, we must reshape the domain for predicting). \n",
    "```python\n",
    "domain = np.linspace(np.min(X) - .5, np.max(X) + .5, 100)\n",
    "\n",
    "\n",
    "plt.figure(figsize = (10, 8))\n",
    "plt.scatter(X, y, color = \"lightseagreen\", label = \"setosa flowers\")\n",
    "plt.plot(domain, node.predict(domain.reshape(-1, 1)))\n",
    "plt.xlabel(\"sepal length [cm]\", fontsize = 15)\n",
    "plt.ylabel(\"sepal width [cm]\", fontsize = 15)\n",
    "plt.title(\"Setosa Regression Data\", fontsize = 18)\n",
    "plt.legend(fontsize = 15)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = np.linspace(np.min(X) - .5, np.max(X) + .5, 100)\n",
    "\n",
    "\n",
    "plt.figure(figsize = (10, 8))\n",
    "plt.scatter(X, y, color = \"lightseagreen\", label = \"setosa flowers\")\n",
    "plt.plot(domain, node.predict(domain.reshape(-1, 1)))\n",
    "plt.xlabel(\"sepal length [cm]\", fontsize = 15)\n",
    "plt.ylabel(\"sepal width [cm]\", fontsize = 15)\n",
    "plt.title(\"Setosa Regression Data\", fontsize = 18)\n",
    "plt.legend(fontsize = 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The line generated by our custom ```SingleNeuron``` class surely does not look random! We can verify this by plotting the ```errors_``` attribute that we stored while training. Note that the $i$-th entry of the ```errors_``` attribute is the *mean-sqaured error* of the neuron after the $i+1$ epoch of stochastic gradient descent. If the mean sqaured error is decreasing after each epoch we are on the right track, and our single neuron might be learning! \n",
    "\n",
    "We can visualize the mean-sqaured error at each epoch of our training process by running the following code in the cell below. \n",
    "```python\n",
    "plt.figure(figsize = (10, 8))\n",
    "plt.plot(range(1, len(node.errors_) + 1), \n",
    "         node.errors_,\n",
    "         marker = \"o\",\n",
    "         label = \"MSE\")\n",
    "plt.xlabel(\"epochs\", fontsize = 15)\n",
    "plt.ylabel(\"MSE\", fontsize = 15)\n",
    "plt.xticks(range(1, len(node.errors_) + 1))\n",
    "plt.legend(fontsize = 15)\n",
    "plt.title(\"MSE Error at Each Epoch During Training\", fontsize = 18)\n",
    "plt.show()\n",
    "\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 8))\n",
    "plt.plot(range(1, len(node.errors_) + 1), \n",
    "         node.errors_,\n",
    "         marker = \"o\",\n",
    "         label = \"MSE\")\n",
    "plt.xlabel(\"epochs\", fontsize = 15)\n",
    "plt.ylabel(\"MSE\", fontsize = 15)\n",
    "plt.xticks(range(1, len(node.errors_) + 1))\n",
    "plt.legend(fontsize = 15)\n",
    "plt.title(\"MSE Error at Each Epoch During Training\", fontsize = 18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### - Visualize your Errors over each Epoch\n",
    "\n",
    "The mean-sqaured error is decreasing over each epoch! Next lets see what happens when we  training a single neuron over 10 times more epochs than before, while leaving the learning rate unchanged. Try running the following code in the cell below (note the use of the subplots). \n",
    "```python\n",
    "node = SingleNeuron(linear_activation)\n",
    "node.train(X, y, alpha = .0001, epochs = 50)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,10))\n",
    "\n",
    "ax1.scatter(X, y, color = \"lightseagreen\")\n",
    "ax1.plot(domain, node.predict(domain.reshape(-1, 1)))\n",
    "ax1.set_xlabel(\"sepal length [cm]\", fontsize = 15)\n",
    "ax1.set_ylabel(\"sepal width [cm]\", fontsize = 15)\n",
    "ax1.set_title(\"Setosa Regression Data\", fontsize = 18)\n",
    "\n",
    "ax2.plot(range(1, len(node.errors_) + 1), \n",
    "         node.errors_,\n",
    "         marker = \"o\",\n",
    "         label = \"MSE\")\n",
    "ax2.set_xlabel(\"epochs\")\n",
    "ax2.set_ylabel(\"MSE\")\n",
    "ax2.set_xticks(range(1, len(node.errors_) + 1, 5))\n",
    "ax2.legend(fontsize = 10)\n",
    "ax2.set_title(\"MSE Error at Each Epoch During Training\", fontsize = 18)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node = SingleNeuron(linear_activation)\n",
    "node.train(X, y, alpha = .0001, epochs = 50)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,10))\n",
    "\n",
    "ax1.scatter(X, y, color = \"lightseagreen\")\n",
    "ax1.plot(domain, node.predict(domain.reshape(-1, 1)))\n",
    "ax1.set_xlabel(\"sepal length [cm]\", fontsize = 15)\n",
    "ax1.set_ylabel(\"sepal width [cm]\", fontsize = 15)\n",
    "ax1.set_title(\"Setosa Regression Data\", fontsize = 18)\n",
    "\n",
    "ax2.plot(range(1, len(node.errors_) + 1), \n",
    "         node.errors_,\n",
    "         marker = \"o\",\n",
    "         label = \"MSE\")\n",
    "ax2.set_xlabel(\"epochs\")\n",
    "ax2.set_ylabel(\"MSE\")\n",
    "ax2.set_xticks(range(1, len(node.errors_) + 1, 5))\n",
    "ax2.legend(fontsize = 10)\n",
    "ax2.set_title(\"MSE Error at Each Epoch During Training\", fontsize = 18)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4.2.3 Experiment with the Learning Rate \n",
    "As discussed in a previous notebook, the choice of learning rate is a crucial *hyperparamter* when implementing gradient (and stochastic gradient) descent. We can view different choices of learning rate with a fixed number of epochs by running the following code in the cell below.\n",
    "```python\n",
    "# Possible choices of learning rate \n",
    "alphas = [0.01, 0.05, 0.07, 0.09]\n",
    "\n",
    "domain = np.linspace(np.min(X) - .5, np.max(X) + .5, 100)\n",
    "\n",
    "# Call the subplots method for plotting a grid of figures\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10,10))\n",
    "\n",
    "# Loop over the axs and alpha values\n",
    "for ax, alpha in zip(axs.flat, alphas):\n",
    "    node = SingleNeuron(linear_activation)\n",
    "    node.train(X, y, alpha = alpha, epochs = 55)\n",
    "    ax.plot(domain, node.predict(domain.reshape(-1, 1)))\n",
    "    ax.scatter(X, y, color = \"lightseagreen\")\n",
    "    ax.set_title(f\"alpha = {alpha}\", fontsize = 18)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possible choices of learning rate \n",
    "alphas = [0.01, 0.05, 0.07, 0.09]\n",
    "\n",
    "domain = np.linspace(np.min(X) - .5, np.max(X) + .5, 100)\n",
    "\n",
    "# Call the subplots method for plotting a grid of figures\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10,10))\n",
    "\n",
    "# Loop over the axs and alpha values\n",
    "for ax, alpha in zip(axs.flat, alphas):\n",
    "    node = SingleNeuron(linear_activation)\n",
    "    node.train(X, y, alpha = alpha, epochs = 1_000)\n",
    "    ax.plot(domain, node.predict(domain.reshape(-1, 1)))\n",
    "    ax.scatter(X, y, color = \"lightseagreen\")\n",
    "    ax.set_title(f\"alpha = {alpha}\", fontsize = 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
