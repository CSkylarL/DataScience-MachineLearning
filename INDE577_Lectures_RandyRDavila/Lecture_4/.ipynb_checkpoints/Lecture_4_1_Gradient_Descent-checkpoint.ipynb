{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Lecture 4.1. Gradient Descent\n",
    "\n",
    "As observed in a previous lecture, we may train a single neurons weights and bias by iteratively applying a predefined rule. The description that we gave for the perceptron update rule was intended to hint at a more general rule for *optimizing the cost function of a model*. More specifically, we were hinting at a general continuous optimization technique called **gradient descent**. \n",
    "\n",
    "\n",
    "## 4.1.1  Minimizing Function of Single Variable\n",
    "In order to understand gradient descent we will need to import the following python packages and define a simple function of one variable. \n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Set Seaborn theme for plots\n",
    "sns.set_theme()\n",
    "\n",
    "# Define test function for experimenting\n",
    "def f(w):\n",
    "    return (w - 2)**2  + 1\n",
    "\n",
    "# Define domain variables for plotting f\n",
    "domain = np.linspace(-2, 6, 50)\n",
    "\n",
    "# Plot the function f(w)\n",
    "plt.figure(figsize = (10, 8))\n",
    "plt.plot(domain, f(domain), label = \"f(w) = (w - 2)^2 + 1\")\n",
    "plt.xlabel(\"w (weight)\", fontsize = 15)\n",
    "plt.ylabel(\"f(w)\", fontsize = 15)\n",
    "plt.legend(fontsize = 15, loc = \"upper center\")\n",
    "plt.title(\"$\\min_w$ $f(w)$ Example\", fontsize = 18)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4.1.1 The Problem Description\n",
    "\n",
    "Clearly the function $f(w) = (w - 2)^2 + 1$ has a **global minimum** at $w = 2$. Supposing we did not already know the answer, how could we find it? That is, we wish to solve:\n",
    "\n",
    "$$\n",
    "\\min_{w\\in \\mathbb{R}} f(w) \\:\\:\\:\\:\\:\\:\\:\\:\\:\\: (\\text{PROBLEM 1})\n",
    "$$\n",
    "\n",
    "This class of problems falls into the reahlm of *unconstrained continuous optimization*. For those of you that are more mathematically inclined, see the excellent and classic text of [Nocedal and Wright](http://egrcc.github.io/docs/math/numerical-optimization.pdf) for an in-depth treatment of the subject.\n",
    "\n",
    "The treatment of PROBLEM 1 given by Nocedal and Wright is extensive and goes much further than what is currently implemented in machine learning; with some notable exceptions appearing in the literature. Noting these techniques, this notebook focuses on the notion of using the *gradient* (the generalization of the single variable function derivative) in order to \"search\" for plausable minimum of a function. \n",
    "\n",
    "### - Start with a Guess\n",
    "Suppose we first guess that the minimum value of $f(w)$ occurs at $w_0 = 5$. We can visualize the point $(5, f(5))$ by running the following code in the cell below. \n",
    "```python\n",
    "plt.figure(figsize = (10, 8))\n",
    "plt.plot(domain, f(domain), label = \"$f(w) = (w - 2)^2 + 1$\")\n",
    "plt.scatter([5], [f(5)], color = \"magenta\", label = \"$w_0$: initial guess\")\n",
    "plt.xlabel(\"w (weight)\", fontsize = 15)\n",
    "plt.ylabel(\"f(w)\", fontsize = 15)\n",
    "plt.legend(fontsize = 15, loc = \"upper center\")\n",
    "plt.title(\"$\\min_w$ $f(x)$ Example\", fontsize = 18)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "### - The General Idea \n",
    "The general idea behind gradient descent is to use the gradient (the derivative for single variable functions) to *provide a direction to explore* (this means gradient descent is a **first-order method**). For example, with our function $f$ and initial guess $w_0 = 5$, suppose we are able to calculate the value of the *gradient* (the derivative) of $f$ at $w_0 = 5$. This numerical value will give us the *slope of the tangent line* to $f$ at $w_0$. Note that $f'(w) = 2(w - 2)$.\n",
    "\n",
    "As a very relevant example, I would like to mention that I am currently recovering from surgery 6 weeks ago on a broken fibula (smaller leg bone). Recently I started walking around (sorta), and whenever I encounter a new incline, I most definitely remember that I'm trying to optimize a function... \n",
    "\n",
    "Specifically, notice that when my character is standing on a 2-dimensional incline it seems obvious how to minimize the pain incured by a step incline. This intuitive notion is mathematically calculated by understanding the slope of the tangent line to the curve I'm currently standing on... \n",
    "\n",
    "Me in Spring 2022\n",
    "---\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"my_loss_function.png\" width=\"700\" hight =\"800\">\n",
    "</p>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "We can even visualize this tangent line by running the following code in the cell below. \n",
    "```python\n",
    "# Define parabola derivative\n",
    "def df(w): \n",
    "    return 2*(w - 2)\n",
    "\n",
    "# Choose w_0\n",
    "w_0 = 5.0\n",
    "\n",
    "# Define tangent line function for visualization\n",
    "def tangent_line(w_i, function, derivative, i = 0, color = \"magenta\", show = True):\n",
    "    # Define tangent line\n",
    "    # y = m*(x - x1) + y1\n",
    "    def line(w):\n",
    "        return derivative(w_i)*(w - w_i) + function(w_i)\n",
    "    \n",
    "    wrange = np.linspace(w_i - 1, w_i + 1, 10)\n",
    "    if show:\n",
    "        plt.plot(wrange,\n",
    "            line(wrange), \n",
    "            'C1--', \n",
    "            linewidth = 1, \n",
    "            color = \"red\", \n",
    "            label = \"tangent line\")\n",
    "    plt.scatter([w_i], [function(w_i)], color = color, label = f\"w_{i}\")\n",
    "\n",
    "# Plot the figure\n",
    "plt.figure(figsize = (10, 8))\n",
    "# Plot the function\n",
    "plt.plot(domain, f(domain), label = \"$f(w) = (w - 2)^2 + 1$\")\n",
    "# Visualize the tangent line\n",
    "tangent_line(w_0, f, df)\n",
    "\n",
    "# Label the figure\n",
    "plt.xlabel(\"w (weight)\", fontsize = 15)\n",
    "plt.ylabel(\"f(w)\", fontsize = 15)\n",
    "plt.legend(fontsize = 15)\n",
    "plt.title(\"$\\min_w$ $f(x)$ Example\", fontsize = 18)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4.1.2 Direction of Descent and the Learning Rate\n",
    "\n",
    "Observing the figure generated by the code above, notice that the tangent line implies a direction of descent. That is, at the point $(5, f(5))$, the tangent line has a positive slope. This indicates that we need move in the negative direction (to the left of $w_0 = 5$) if we wish to move to a smaller value of $f(w)$. That is, **we need to move in the direction opposite of the sign of the derivative of $f(w)$ at $w_0 = 5$**.\n",
    "\n",
    "### - But how far should we move? \n",
    "The value of how far to move in the opposite sign of the derivative of $f(w)$ at $w_0 = 5$ is called the **learning rate** (Nocedal & Wright call this *hyperparameter* the **step length**), and is typically denoted by $\\alpha$. The process of multiplying the derivative of $f(w)$ at $w_0 = 5$ by the learning rate and forming a new choice of $w$ by subtracting this quantity from $w_0$ is called **gradient descent**. For example, we may apply gradient descent at $w_0$ and form a new $w$, say $w_1$, with the following update:\n",
    "\n",
    "$$\n",
    "w_{n+1} = w_n - \\alpha f'(w_n) \\:\\:\\:\\:\\:\\:\\:\\:\\:\\: (\\text{Gradient Descent Update Rule for a Function of one Variable})\n",
    "$$\n",
    "\n",
    "The choice of $\\alpha$ in machine learning is typically found by experimentation, though more sophesticated techniques are available, such as *line-search* and *trust-region* methods (again see Nocedal & Wright). \n",
    "\n",
    "We can implement one iteration of the gradient descent algorithm by choosing $\\alpha = 0.8$ and then running the following code in the cell below. \n",
    "```python\n",
    "# Initialize choice of w\n",
    "w_0 = 5.0\n",
    "\n",
    "# Set learning rate \n",
    "alpha = .8\n",
    "\n",
    "# Moving in the opposite direction of the derivative at w_0\n",
    "w_1 = w_0 - alpha*df(w_0)\n",
    "\n",
    "# Plot the figure\n",
    "plt.figure(figsize = (10, 8))\n",
    "plt.plot(domain, f(domain), label = \"$f(w) = (w - 2)^2 + 1$\")\n",
    "\n",
    "# Visualize the tangent lines\n",
    "tangent_line(w_0, f, df, show = False)\n",
    "tangent_line(w_1, f, df, i = 1, color = \"green\")\n",
    "\n",
    "# Plot labels\n",
    "plt.xlabel(\"w (weight)\", fontsize = 15)\n",
    "plt.ylabel(\"f(w)\", fontsize = 15)\n",
    "plt.legend(fontsize = 15, loc = \"upper center\")\n",
    "plt.title(\"$\\min_w$ $f(x)$\", fontsize = 18)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Notice that the new guess $w_1$ gives a new pair $(w_1, f(w_1))$ which is a better choice of a both a *extrema* and minimum value for $f$ than the initial guess $w_0$ would have given. **We are moving downhill on the function $f$.** Let us move again and see where we end up by running the following code in the cell below. \n",
    "```python\n",
    "# Moving in the opposite direction of the derivative at w_0\n",
    "w_2 = w_1 - alpha*df(w_1)\n",
    "\n",
    "# Plot the figure\n",
    "plt.figure(figsize = (10, 8))\n",
    "plt.plot(domain, f(domain), label = \"$f(w) = (w - 2)^2 + 1$\")\n",
    "\n",
    "# Visualize the tangent lines\n",
    "tangent_line(w_0, f, df, show = False)\n",
    "tangent_line(w_1, f, df, i = 1, color = \"green\", show = False)\n",
    "tangent_line(w_2, f, df, i = 2, color = \"red\")\n",
    "\n",
    "# Plot labels\n",
    "plt.xlabel(\"w (weight)\", fontsize = 15)\n",
    "plt.ylabel(\"f(w)\", fontsize = 15)\n",
    "plt.legend(fontsize = 15, loc = \"upper center\")\n",
    "plt.title(\"$\\min_w$ $f(x)$\", fontsize = 18)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "And one more time by running the following code in the cell below.\n",
    "```python\n",
    "# Moving in the opposite direction of the derivative at w_0\n",
    "w_3 = w_2 - alpha*df(w_2)\n",
    "\n",
    "print(f\"{w_0 = }\")\n",
    "print(f\"{w_1 = }\")\n",
    "print(f\"{w_2 = }\")\n",
    "print(f\"{w_3 = }\")\n",
    "\n",
    "# Plot the figure\n",
    "plt.figure(figsize = (10, 8))\n",
    "plt.plot(domain, f(domain), label = \"$f(w) = (w - 2)^2 + 1$\")\n",
    "\n",
    "# Visualize the tangent lines\n",
    "tangent_line(w_0, f, df, show = False)\n",
    "tangent_line(w_1, f, df, i = 1, color = \"green\", show = False)\n",
    "tangent_line(w_2, f, df, i = 2, color = \"red\", show = False)\n",
    "tangent_line(w_3, f, df, i = 3, color = \"purple\")\n",
    "\n",
    "# Plot labels\n",
    "plt.xlabel(\"w (weight)\", fontsize = 15)\n",
    "plt.ylabel(\"f(w)\", fontsize = 15)\n",
    "plt.legend(fontsize = 15, loc = \"upper center\")\n",
    "plt.title(\"$\\min_w$ $f(x)$\", fontsize = 18)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "We can easily iterate this process of updating $w_i$ by writing a function called ```derivative_descent```. Try running the following code in the cell below. \n",
    "```python\n",
    "def derivative_descent(derivative, alpha = 0.8, w_0 = 5.0, max_iter = 1_000):\n",
    "    W = [w_0]\n",
    "    i = 0\n",
    "    while abs(derivative(W[-1])) > 0.001 and i < max_iter:\n",
    "        w_new = W[-1] - alpha*df(W[-1])\n",
    "        W.append(w_new)\n",
    "        i += 1\n",
    "    W = np.array(W)\n",
    "\n",
    "    return W\n",
    "\n",
    "W = derivative_descent(df)\n",
    "\n",
    "for i, w in enumerate(W):\n",
    "    print(f\"w_{i} = {np.round(w, decimals = 2)} | df(w_{i}) = {df(w)}\")\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "# Plot the figure\n",
    "plt.figure(figsize = (10, 8))\n",
    "plt.plot(domain, f(domain), label = \"$f(w) = (w - 2)^2 + 1$\")\n",
    "plt.scatter(W, f(W), color = \"magenta\")\n",
    "tangent_line(W[-1], f, df, i = len(W), color = \"blue\")\n",
    "plt.xlabel(\"$w$ (weight)\", fontsize = 15)\n",
    "plt.ylabel(\"$f(w)$\", fontsize = 15)\n",
    "plt.legend(fontsize = 15, loc = \"upper center\")\n",
    "plt.title(f\"alpha = {alpha}, Iterations = {len(W)}\", fontsize = 18)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "Me in Spring 2022 After Reaching a Place to Sit!\n",
    "---\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"Valhalla.png\" width=\"700\" hight =\"800\">\n",
    "</p>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### - Testing different choices of the Learning Rate $\\alpha$\n",
    "We can test how well our choice of the learning rate $\\alpha$ was by comparing it to different choices of $\\alpha$. Try running the following code in the cell below. What do you observe?\n",
    "```python\n",
    "# Possible choices of learning rate \n",
    "alphas = [0.1, 0.5, 0.8, 0.9]\n",
    "\n",
    "# Call the subplots method for plotting a grid of figures\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10,10))\n",
    "\n",
    "# Loop over the axs and alpha values\n",
    "for ax, alpha in zip(axs.flat, alphas):\n",
    "    W = derivative_descent(df, alpha = alpha)\n",
    "    ax.plot(domain, f(domain))\n",
    "    ax.scatter(W, f(W), color = \"magenta\")\n",
    "    ax.set_title(f\"alpha = {alpha}, Iterations = {len(W)}\", fontsize = 18)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4.1.3 Minimizing Functions of Several Variables\n",
    "\n",
    "All of the ideas above naturally generalize to functions of several variables when substituting the gradient for the single variable derivative. Before discussing this notion, we emphasize the general uncrontrained optimization problem:\n",
    "\n",
    "$$\n",
    "\\min_{w\\in \\mathbb{R^n}} f(w) \\:\\:\\:\\:\\:\\:\\:\\:\\:\\: (\\text{PROBLEM 2})\n",
    "$$\n",
    "\n",
    "For instructional purposes we next give a specific solution to this problem by focusing on a function of two variables, though all notions covered extend to functions of an arbitrary and finite number of variables. In a general manor, gradient descent can now be defined in a meaningful sense:\n",
    "\n",
    "$$\n",
    "w \\leftarrow w - \\alpha \\nabla f(w) \\:\\:\\:\\:\\:\\:\\:\\:\\:\\: (\\text{True Gradient Descent})\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "With this multivariable gradient descent defined we now consider the multivariable function $f(w_0, w_1) = w_0^2 + w_1^2 + 1$ which has an obvious minimum at the vector $\\mathbf{w} = [w_0, w_1]^T$. To visualize this function run the following code in the cell below.\n",
    "```python\n",
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "def f(w_0, w_1):\n",
    "    return w_0 ** 2 + w_1 ** 2 + 1\n",
    "\n",
    "x = np.linspace(-6, 6, 300)\n",
    "y = np.linspace(-6, 6, 300)\n",
    "\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = f(X, Y)\n",
    "\n",
    "fig = plt.figure(figsize = (10, 8))\n",
    "ax = plt.axes(projection='3d')\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.contour3D(X, Y, Z, 50, cmap='binary')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_zlabel('z')\n",
    "\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The figure shown by the previous code cell can be a bit hard to think about. To work around this problem try running the following code in the cell below. \n",
    "```python\n",
    "x = np.linspace(-6, 6, 300)\n",
    "y = np.linspace(-6, 6, 300)\n",
    "\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = f(X, Y)\n",
    "\n",
    "fig = plt.figure(figsize = (10, 8))\n",
    "ax = plt.axes(projection='3d')\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.contour3D(X, Y, Z, 50, cmap='binary')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_zlabel('z')\n",
    "\n",
    "\n",
    "ax.view_init(60, 35)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### - True Gradient Descent \n",
    "Now that we have a good visualization of our multivariable function we next define the derivative of our function and also update our gradient descent function to work with such functions. Next run the following code in the cell below. \n",
    "```python\n",
    "def df(w):\n",
    "    grad = [2*w[0], 2*w[1]]\n",
    "    return np.array(grad)\n",
    "\n",
    "def gradient_descent(derivative, W, alpha = 0.8, w_0 = 5.0, max_iter = 1_000):\n",
    "    i = 0\n",
    "    W_hist = [W]\n",
    "    while i < max_iter:\n",
    "        W = W - alpha*df(W)\n",
    "        i += 1\n",
    "        W_hist.append(W)\n",
    "\n",
    "    return W_hist\n",
    "\n",
    "W = gradient_descent(df, np.array([5.0, -5.0]), alpha = 0.1, max_iter=500)\n",
    "\n",
    "x = np.linspace(-6, 6, 100)\n",
    "y = np.linspace(-6, 6, 100)\n",
    "\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = f(X, Y)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize = (10, 8))\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.contour3D(X, Y, Z, 50, cmap='binary')\n",
    "for i in range(len(W)):\n",
    "    ax.scatter3D([W[i][0]], \n",
    "                 [W[i][1]], \n",
    "                 [f(W[i][0], W[i][1])], color = \"blue\")\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_zlabel('z')\n",
    "ax.view_init(60, 35)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4.1.4 Conclusion\n",
    "\n",
    "As we have demonstrated in this notebook, gradient descent is a simple way to optimize convex functions. In the next notebook we show how this iterative method can be used to train single neuron models! \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
