{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Lecture 10.2 Principal Component Analysis (PCA)\n",
    "\n",
    "**Principal Component Analysis** (PCA) is used in exploratory data analysis and for multidimensionality reduction. The main idea is to project data points onto only the first few principal components to obtain lower-dimensional data while preserving as much of the data’s variation as possible. In other words, using PCA we remove the redundant and highly-correlated data and we keep only the most significant data features for further analysis.\n",
    "\n",
    "The first principal component is defined as a direction that maximizes variance of the projected data, the second principal component is a direction orthogonal to the first principal component that is the next one to maximize the variance, etc. It can be proved that the principal components are the eigenvectors of the covariance matrix and are computed either by eigendecomposition of the covariance matrix or by the SVD of the data matrix.\n",
    "\n",
    "Assume we have data consisting of $m$ variables (or features, or attributes, such as age, height, weight, income, etc.) and n observations (or data points, or samples). We form the $m \\times n$ ”feature - observation” matrix where variables are listed in the rows and observations in the columns. Some authors and Python prefer ”observation – feature” matrix instead of ”feature – observation” matrix. \n",
    "\n",
    "\n",
    "As before, we will use the iris dataset to demonstrate this new technique. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "iris = sns.load_dataset(\"iris\")\n",
    "\n",
    "X = iris[[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]].to_numpy()\n",
    "y = iris.species.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The goal of PCA is to reduce the dimensionality of the feature vectors used in training machine learning algorithms. The steps in PCA are:\n",
    "\n",
    "1. **Stadardize (center and scale) the data.** \n",
    "\n",
    "To center the data, we average each row by replacing the value $x$ by \n",
    "$$\n",
    "x - \\text{mean}\n",
    "$$\n",
    "Data values may have vastly different ranges, and so, to ensure that PCA is not selecting wrong directions in describing data variation, we also divide by the standard deviation. That is, we scale the data in each variable by finding the *$z$-scores*:\n",
    "\n",
    "$$\n",
    "z = \\frac{x - \\text{mean}}{\\text{standard devation}}\n",
    "$$\n",
    "\n",
    "Finally, we form the $m\\times n$ matrix $A$. \n",
    "\n",
    "2. **Compute the covariance or correlation matrix**:\n",
    "\n",
    "$$\n",
    "S = \\frac{1}{n-1}AA^T\n",
    "$$\n",
    "\n",
    "If we are working with only centered data, the above matrix is the covariance matrix, and if we are working with scaled data, then $S$ is the correlation matrix. The entries on the diagonal are the variances (or correlations) for each variable and the off-diagonal entries are the covariances (or correlations) between two variables: positive covariance indicates that the variables are directly related (when one increases, the other increases as well), negative covariance indicates inverse relationship (when one increases, the other decreases). This matrix is symmetric of size $m \\times m$, so its columns are of the same size as the columns of $A$.\n",
    "\n",
    "3. **Find the eigenvalues and the orthonormal eigenvectors of $S$.** \n",
    "\n",
    "These eigenvectors are columns of the matrix $U$ in the singular value decomposition of $A$, up to the factor $n-1$. Further, we denote the eigenvalues by $\\sigma_{i}^{2}$. This is equivalent to the **Singular Value Decomposition** of our shifted training set matrix $A$,\n",
    "\n",
    "$$\n",
    "A = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^{T}, \n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "A = X - X.mean(axis = 0)\n",
    "\n",
    "U, sigma, Vt = np.linalg.svd(A, full_matrices=False)\n",
    "\n",
    "print(f\"np.shape(U) = {np.shape(U)}\")\n",
    "print(f\"np.shape(sigma) = {np.shape(sigma)}\")\n",
    "print(f\"np.shape(Vt) = {np.shape(Vt)} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_mat = np.diag(sigma)\n",
    "print(f\"A == U * sigma_mat * Vt: {np.allclose(A, np.dot(U, np.dot(sigma_mat, Vt)))} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "4. **Find the principal components.**\n",
    "\n",
    "We arrange the eigenvalues found in the previous step in the decreasing order. The first principal component $PC_1$ is in the direction of the 1st eigenvector, the second principal component $PC_2$ is in the direction of the 2nd eigenvector, etc. The entries of each $PC_i$ are called *loading scores* and they tell us how the $PC_i$ is a linear combination of features.\n",
    "\n",
    "5. **Reduce the dimension of the data.**\n",
    "\n",
    "We project data points (i.e., columns of $A$) onto the selected principal components (i.e., several eigenvectors of $S$). By the Eckart-Young theorem we know that the line closest to the data points is in the direction of $PC_1$, etc (”closest” is in the sense of perpendicular least squares).\n",
    "\n",
    "In addition, the total variance, which is the trace of $S$, is\n",
    "\n",
    "$$\n",
    "T = \\text{trace}(S) = \\frac{\\sigma_{1}^{2} + \\dots + \\sigma_{m}^{2}}{n-1},\n",
    "$$\n",
    "\n",
    "and the $i$-th principle component $PC_i$ explains\n",
    "\n",
    "$$\n",
    "\\frac{\\sigma_{i}^{2}/(n-1)}{T} = \\frac{\\sigma_{i}^{2}}{\\sigma_{1}^{2} + \\dots + \\sigma_{m}^{2}}\n",
    "$$\n",
    "\n",
    "of the total variation. We use a scree plot to graph the percentages of variation that each $PC_i$ accounts for. Also, the sum of squared distances from the points projected to $PC_i$ to the origin is the eigenvalue for $PC_i$ or the squared singular value $\\sigma_{i}^{2}$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "To project the data contained in $A$ onto the first two principle component axis, we compute $A [PC_1 PC_2]$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PC1 = Vt.T[:, 0]\n",
    "PC2 = Vt.T[:, 1]\n",
    "PC3 = Vt.T[:, 2]\n",
    "PC4 = Vt.T[:, 3]\n",
    "\n",
    "W2 = Vt.T[:, :2]\n",
    "X2D = A.dot(W2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colors(y):\n",
    "    if y == \"setosa\":\n",
    "        return \"red\"\n",
    "    elif y == \"versicolor\":\n",
    "        return \"magenta\"\n",
    "    else:\n",
    "        return \"lightseagreen\"\n",
    "\n",
    "c = [colors(label) for label in y]\n",
    "\n",
    "plt.figure(figsize = (10, 8))\n",
    "plt.scatter(X2D[:, 0], X2D[:, 1], c = c)\n",
    "plt.xlabel(\"First Principle Component\", fontsize = 15)\n",
    "plt.ylabel(\"Second Principle Component\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "\n",
    "scaled_X = preprocessing.scale(X)\n",
    "pca = PCA()\n",
    "pca.fit(scaled_X)\n",
    "\n",
    "print(f\"pca.explained_variance_ratio_ = {pca.explained_variance_ratio_}\")\n",
    "\n",
    "per_var = np.round(pca.explained_variance_ratio_*100, 2)\n",
    "print(f\"per_var = {per_var} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bar chart for the above array\n",
    "# This chart is called a \"Scree Plot\"\n",
    "\n",
    "labels = [f\"PC{i}\" for i in range(1,5)]\n",
    "\n",
    "plt.figure(figsize = (10, 8))\n",
    "plt.bar(x = range(1, 5), height = per_var, tick_label = labels)\n",
    "plt.xlabel('Principal Component', fontsize = 15)\n",
    "plt.ylabel('Percentage of Variation', fontsize = 15)\n",
    "plt.title('Scree Plot', fontsize = 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we call function pca.components_ to see how each PC is obtained\n",
    "# as a linear combination of the original coordinates\n",
    "\n",
    "# for example, here PC1 = 0.522 * sepal_length + 0.372 * sepal_width - 0.721 * petal_length - 0.262 * petal_width\n",
    "\n",
    "features = list(set(iris.columns) - {\"species\"})\n",
    "\n",
    "pd.DataFrame(data = pca.components_, columns = labels, index = features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use pca.transform to see how our data looks like in the new coordinate system\n",
    "\n",
    "# this will be used later to plot our data in the first two coordinates PC1, PC2\n",
    "\n",
    "pca_data = pca.transform(scaled_X)\n",
    "\n",
    "pca_df = pd.DataFrame(pca_data, columns = labels)\n",
    "\n",
    "pca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we add to the previous table the labels for each flower\n",
    "\n",
    "projected_df = pd.concat([pca_df, iris.species], axis = 1)\n",
    "projected_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projected_df[\"color\"] = c\n",
    "projected_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [\"setosa\", \"versicolor\", \"virginica\"]\n",
    "colors = [\"red\", \"magenta\", \"lightseagreen\"]\n",
    "\n",
    "plt.figure(figsize = (10, 8))\n",
    "\n",
    "for target, color in zip(targets, colors):\n",
    "    temp_df = projected_df[projected_df['species'] == target]\n",
    "    plt.scatter(temp_df[\"PC1\"],\n",
    "                temp_df[\"PC2\"],\n",
    "                c = color)\n",
    "    \n",
    "plt.xlabel('PC1', fontsize = 15)\n",
    "plt.ylabel('PC2', fontsize = 15)\n",
    "plt.title('Two-component PCA', fontsize = 18)\n",
    "plt.legend(targets)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "cancer = load_breast_cancer(as_frame = True)\n",
    "cancer.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = cancer[\"feature_names\"]\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = cancer[\"target_names\"]\n",
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_df = cancer[\"frame\"]\n",
    "cancer_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cancer_df[features].to_numpy()\n",
    "scaled_X = preprocessing.scale(X)\n",
    "scaled_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()   # perform PCA and get 30 new coordinates that we call principal components (PCs)\n",
    "pca.fit(scaled_X)\n",
    "\n",
    "\n",
    "print(f\"pca.explained_variance_ratio_ = {pca.explained_variance_ratio_}\")\n",
    "\n",
    "per_var = np.round(pca.explained_variance_ratio_*100, 2)\n",
    "print(f\"per_var = {per_var} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [f\"PC{i}\" for i in range(1,31)]\n",
    "\n",
    "plt.figure(figsize = (10, 8))\n",
    "plt.bar(x = range(1, 31), height = per_var, tick_label = labels)\n",
    "plt.xlabel('Principal Component', fontsize = 15)\n",
    "plt.ylabel('Percentage of Variation', fontsize = 15)\n",
    "plt.title('Scree Plot', fontsize = 18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this shows how each PC can be obtained using the original coordinates\n",
    "\n",
    "# PC1 = 0.219 * mean radius - 0.234 * mean texture ......\n",
    "\n",
    "labels = [f\"PC{i}\" for i in range(1, 31)]\n",
    "\n",
    "pd.DataFrame(pca.components_, columns = labels, index = cancer['feature_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_data = pca.transform(scaled_X)\n",
    "\n",
    "pca_df = pd.DataFrame(pca_data, columns = labels)\n",
    "\n",
    "pca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def color(label):\n",
    "    if label == 0:\n",
    "        return \"magenta\"\n",
    "    else:\n",
    "        return \"lightseagreen\"\n",
    "\n",
    "colors = [color(label) for label in cancer_df[\"target\"]]\n",
    "\n",
    "plt.figure(figsize = (10, 8))\n",
    "plt.scatter(pca_df.PC1, pca_df.PC2, c = colors)\n",
    "plt.xlabel(\"PC1\", fontsize = 15)\n",
    "plt.ylabel(\"PC2\", fontsize = 15)\n",
    "plt.title(\"Two-component PCA\", fontsize = 18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
